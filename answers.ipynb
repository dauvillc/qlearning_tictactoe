{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58fa93b-5307-40b0-9b87-1896707c25b9",
   "metadata": {},
   "source": [
    "# Mini-project 1: Tic-Tac-Toe\n",
    "ClÃ©ment DAUVILLIERS - Florian VINCENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14bd7b-e0ed-415d-a44f-a013a1b5ca3a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250cc033-1d30-4b7f-9f5b-1314532de708",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b31125c7-3f29-43bb-a467-9caaa336a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange\n",
    "from math import log\n",
    "from copy import deepcopy\n",
    "from queue import deque\n",
    "from random import sample\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "import numpy.typing as npt\n",
    "from typing import List, Tuple, NewType, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86870d16-c73e-44e9-97fe-ad9346f7c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid = NewType('Grid', npt.NDArray[np.float64])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b2b5e6-3607-4252-b997-67dcb8d1646a",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e20fe206-dcef-43bb-865c-f7d9dfda47c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_avg(arr: npt.ArrayLike, window_len: int=250) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Computes the average over successive windows of an array.\n",
    "    arr must be a 1D array whose length is a multiple of the\n",
    "    window length.\n",
    "    \"\"\"\n",
    "    return arr.reshape((window_len, -1)).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c32c475-22d4-4e03-9dac-d32b5adef9eb",
   "metadata": {},
   "source": [
    "## Player class\n",
    "The following class will be used as base for the QLearning and DQN player classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46b11f77-858b-4ce3-a796-c5612e69154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    \"\"\"\n",
    "    Base class for both types of players (QLearning, DQN).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 player: str = 'X',\n",
    "                 epsilon: float = 0.05,\n",
    "                 seed: int = 666):\n",
    "        self.player: str = player\n",
    "        self.epsilon: float = epsilon\n",
    "        \n",
    "        # RNG for the epsilon-gredy policy\n",
    "        self.rng_ = np.random.default_rng(seed=seed)\n",
    "    \n",
    "    def act(self, grid: Grid) -> int:\n",
    "        \"\"\"\n",
    "        Selects an action to perform based on the current\n",
    "        grid state and the player's policy.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Call from abstract class\")\n",
    "        \n",
    "    def set_player(self, player: str) -> None:\n",
    "        self.player = player\n",
    "    \n",
    "    @staticmethod\n",
    "    def empty(grid: Grid) -> List[Tuple[int, int]]:\n",
    "        '''return all empty positions'''\n",
    "        avail = []\n",
    "        for i in range(9):\n",
    "            pos = (int(i/3), i % 3)\n",
    "            if grid[pos] == 0:\n",
    "                avail.append(pos)\n",
    "        return avail\n",
    "        \n",
    "    def randomMove(self, grid: Grid) -> Tuple[int, int]:\n",
    "        \"\"\" Chose a random move from the available options. \"\"\"\n",
    "        avail = self.empty(grid)\n",
    "\n",
    "        return avail[self.rng_.integers(0, len(avail))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1499d2e6-ceb4-4b41-93b3-f7459700b195",
   "metadata": {},
   "source": [
    "# 2. Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb32df3e-82d9-4c5d-8fbe-2b7d7b19ebe0",
   "metadata": {},
   "source": [
    "### QLPlayer class\n",
    "The following class implements the QLearning player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22d931d5-1c3d-46e8-b6b1-d15ffcbfb2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLPlayer(Player):\n",
    "    \"\"\"\n",
    "    Implements a player that learns using the QLearning algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, player='X',\n",
    "                 lr: float = 0.05,\n",
    "                 discount: float = 0.99,\n",
    "                 epsilon: float = 0.05,\n",
    "                 seed: int = 666):\n",
    "        super().__init__(player, epsilon, seed)\n",
    "        self.lr: float = lr\n",
    "        self.discount: float = discount\n",
    "        self.epsilon: float = epsilon\n",
    "        \n",
    "        # Q-values grid\n",
    "        # 3^9 = 19683 states and 9 actions\n",
    "        self.qvalues: Grid = np.zeros((19683, 9))\n",
    "        \n",
    "        # Memory\n",
    "        self.last_action: Union[None, int] = None\n",
    "        self.last_state: Union[None, Grid] = None\n",
    "        \n",
    "    def act(self, grid):\n",
    "        # Inverts the grid if required\n",
    "        grid = self.invert_grid(grid)\n",
    "        \n",
    "        # Epsilon-greedy choice\n",
    "        if self.rng_.random() < self.epsilon:\n",
    "            return self.randomMove(grid)\n",
    "        # Retrieves the list of possible actions and converts them\n",
    "        # from cell positions to integer indexes\n",
    "        avail_actions: List[int] = QLPlayer.positions_to_ints(Player.empty(grid))\n",
    "        # Ranks ALL actions according to their Qvalues in the current\n",
    "        # state\n",
    "        state: int = QLPlayer.state_to_int(grid)\n",
    "        actions_ranks: npt.NDArray = np.argsort(self.qvalues[state])[::-1]\n",
    "        # Browses all actions in order of their qvalue rank, until\n",
    "        # finding one that is available\n",
    "        for action in actions_ranks:\n",
    "            if action in avail_actions:\n",
    "                # Memorizes the action and the current state for the learning\n",
    "                # phase\n",
    "                self.last_action, self.last_state = action, state\n",
    "                return int(action)\n",
    "    \n",
    "    def learn(self, \n",
    "              reward: int,\n",
    "              new_grid: Grid,\n",
    "              end: int) -> None:\n",
    "        \"\"\"\n",
    "        Updates the Qvalues based on the last (S, A) pair and\n",
    "        the received reward and the new state.\n",
    "        \"\"\"\n",
    "        # If the new_grid is a final state, we can't compute its expected optimal\n",
    "        # qvalue. We instead set it to zero.\n",
    "        if end:\n",
    "            new_state_qval: int = 0\n",
    "        else:\n",
    "            # Inverts the grid if required (so that ones corresponding\n",
    "            # to THIS player's chesses).\n",
    "            new_grid: Grid = self.invert_grid(new_grid)\n",
    "\n",
    "            # Computes the optimal Qvalue in the new state max Q(s', a)\n",
    "            new_state: int = QLPlayer.state_to_int(new_grid)\n",
    "            new_state_qval: np.float64 = np.max(self.qvalues[new_state])\n",
    "        \n",
    "        # QValue that needs to be updated Q(s, a)\n",
    "        current_qval: np.float64 = self.qvalues[self.last_state, self.last_action]\n",
    "        \n",
    "        self.qvalues[self.last_state, self.last_action] += self.lr * (reward + self.discount * new_state_qval - current_qval)\n",
    "    \n",
    "    def invert_grid(self, grid: Grid) -> Grid:\n",
    "        \"\"\"\n",
    "        Returns a version of the grid in which the ones correspond to this player's\n",
    "        chesses.\n",
    "        \"\"\"\n",
    "        # If we play with the 'O', then the -1 in the grid are actually our pieces\n",
    "        return grid if self.player == 'X' else -grid\n",
    "    \n",
    "    @staticmethod\n",
    "    def position_to_int(position: Tuple[int, int]) -> int:\n",
    "        \"\"\"\n",
    "        (row col) -> row*3 + col\n",
    "        \"\"\"\n",
    "        return position[0] * 3 + position[1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def positions_to_ints(positions: List[Tuple[int, int]]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Given a list of cells [(a, b), (c, d), ..],\n",
    "        returns the list of the corresponding indexes.\n",
    "        \"\"\"\n",
    "        return [QLPlayer.position_to_int(cell) for cell in positions]\n",
    "\n",
    "    @staticmethod\n",
    "    def state_to_int(grid: Grid) -> int:\n",
    "        \"\"\"\n",
    "        Converts a grid state to the index of its\n",
    "        row in the lookup table.\n",
    "        \"\"\"\n",
    "        # Converts the grid values from -1, 0, 1 to 0, 1, 2 (a base 3 number)\n",
    "        # Then converts the base 3 number to base 10\n",
    "        return int((np.ravel(grid) + 1) @ np.array([3 ** i for i in range(9)]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def int_to_state(state_int: int) -> Grid:\n",
    "        \"\"\"\n",
    "        Converts the index of row in the qvalues table to\n",
    "        its corresponding state.\n",
    "        \"\"\"\n",
    "        # Converts from base 10 to base 3\n",
    "        return np.array([\n",
    "            (state_int % (3 ** (i + 1))) // (3 ** i)\n",
    "            for i in range(9)\n",
    "        ]).reshape((3, 3)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b2b39-d3e1-4de5-abf7-6fd73cd57acf",
   "metadata": {},
   "source": [
    "## 2.1 Learning from experts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96654ae4-a652-4435-945b-5a82a533c250",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ea95936-f056-43f1-93af-1072a940bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_games(learning_player: Union[Player, OptimalPlayer],\n",
    "               benchmark_player: Union[Player, OptimalPlayer],\n",
    "               nb_games: int = 20000,\n",
    "               turns_swap: str = \"switch\",\n",
    "               seed: int = 666,\n",
    "               learn: bool = True) -> npt.NDArray[np.int_]:\n",
    "    \"\"\"\n",
    "    Plays a given number of games between two players, and returns the rewards.\n",
    "    --learning_player: Player object implementing act(), learn(), update();\n",
    "    --benchmark_player: Player object implementing act();\n",
    "    --nb_games: How many games should be played;\n",
    "    --turns_swap: str, either \"switch\" to switch turns after every game, or \"random\".\n",
    "    --seed: random seed.\n",
    "    \"\"\"\n",
    "    turns: npt.NDArray[np.string_] = np.array(['X','O'])\n",
    "    learning_player.set_player(turns[0])\n",
    "    benchmark_player.set_player(turns[1])\n",
    "    rewards: List[int] = []\n",
    "    env = TictactoeEnv()\n",
    "    \n",
    "    for game in trange(nb_games):\n",
    "        # Sets up the environment for the game\n",
    "        env.reset()\n",
    "        grid: Grid = env.observe()[0]\n",
    "        if turns_swap == \"switch\":\n",
    "            turns = turns[[-1, 0]]\n",
    "        else:\n",
    "            turns = np.random.shuffle(turns)\n",
    "        learning_player.set_player(turns[0])\n",
    "        benchmark_player.set_player(turns[1])\n",
    "        \n",
    "        while True:\n",
    "            # Action step\n",
    "            if env.current_player == learning_player.player:\n",
    "                move: int = learning_player.act(grid)\n",
    "            else:\n",
    "                move: int = benchmark_player.act(grid)\n",
    "\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "            reward: int = env.reward(learning_player.player)\n",
    "\n",
    "            # Learning step\n",
    "            # The agent learns only after the other has played, as from the\n",
    "            # point of view of the agent, the next state is not the one right after\n",
    "            # its move, but the next state in which the agent will need to make a decision.\n",
    "            if (env.current_player == benchmark_player.player or end) and learn:\n",
    "                learning_player.learn(reward, grid, end)\n",
    "\n",
    "            if end:\n",
    "                env.reset()\n",
    "                rewards.append(reward)\n",
    "                break\n",
    "    \n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e865c195-e3d9-4a6a-9603-c233a9471915",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlplayer = QLPlayer(epsilon=0.1)\n",
    "semi_random_player = OptimalPlayer(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc53ea42-3aad-44b6-abd6-55e7aab286ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5521ebfdc0574c858a937ecb528c1cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards_21 = play_games(qlplayer, semi_random_player, nb_games=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a3a7434-9f97-4867-8cfc-ca93c850d876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(windowed_avg(rewards_21))\n",
    "plt.ylim(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5242a9-7f44-4147-80de-4bd8ca20a9a0",
   "metadata": {},
   "source": [
    "# 3. Deep Q-Learning\n",
    "All of the following implementations will be based on the PyTorch RL Tutorial:  \n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.htmlhttps://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ce705-50db-4f79-9ca4-c1b582865869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79746898-a689-4fd0-abee-6c7897c628cf",
   "metadata": {},
   "source": [
    "We'll try to work on a GPU if one is available to PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27fc3e-b8aa-4a4c-b391-7ebecf264f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315c8eb-2938-48a3-8959-507049afcd7c",
   "metadata": {},
   "source": [
    "## Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f38b6-dda0-4e27-ae44-498ac117aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    state (3,3,2)->flatten->(18,) nn input\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.input = nn.Linear(18, 128)\n",
    "        self.hidden1 = nn.Linear(128, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.output = nn.Linear(128, 9)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        # Flattens x to make sure it can be passed to the linear layers\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        # No activation is a linear activation\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2681db-9713-440a-bbea-73a865d205d9",
   "metadata": {},
   "source": [
    "## Replay memory\n",
    "This class is directly taken from the PyTorch DQN tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf7201-8a61-456b-a638-eff02c09e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5455aa26-ee1a-4d0b-b329-284c6b75278f",
   "metadata": {},
   "source": [
    "## DQNPlayer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f19284-361f-400f-9f62-66dedd5e9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNPlayer(Player):\n",
    "    \"\"\"\n",
    "    Implements a type of Player that uses Deep Q-Learning\n",
    "    to learn the tictactoe strategy.\n",
    "    \"\"\"\n",
    "    def __init__(self, player='X', lr=5e-4, discount=0.99, epsilon=0.05, batch_size=64,\n",
    "                  seed=666):\n",
    "        super().__init__(player, epsilon, seed)\n",
    "        self.lr = lr\n",
    "        self.discount = discount\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.last_action, self.last_state = None, None\n",
    "        \n",
    "        # Neural networks\n",
    "        self.policy_net = DQN().to(device)\n",
    "        self.target_net = DQN().to(device)\n",
    "        \n",
    "        self.memory = ReplayMemory(10000)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Huber loss\n",
    "        self.criterion = nn.HuberLoss()\n",
    "        # Adam optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
    "    \n",
    "    def learn(self, reward, grid):\n",
    "        \"\"\"\n",
    "        Stores the last (S, A, NS, R) tuple into the replay memory,\n",
    "        and trains the policy network using a sample from the replay memory.\n",
    "        --reward: float, end game reward\n",
    "        --grid: (3, 3, 2) array representing the current state.\n",
    "        Returns the value of the Huber loss.\n",
    "        \"\"\"\n",
    "        # Push the last experience into the replay memory\n",
    "        self.memory.push(self.last_state, self.last_action, grid, reward)\n",
    "        \n",
    "        # We don't start learning before the replay memory is large enough to\n",
    "        # return at least a full batch\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        ## Policy network training ============================================\n",
    "        # Taken from the Pytorch RL tutorial\n",
    "        # First sample a batch of Transition objects\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # Then creates a single Transition obj whose elements are arrays\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        next_state_batch = torch.cat(batch.next_state)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        # We need to know where in the batch the next state is final\n",
    "        # as we can't compute max Q(ns, .) for those.\n",
    "        # The final states are characterized by having the sum of the grid equal to 9\n",
    "        # (1 for each cell).\n",
    "        non_final_mask = next_state_batch.sum((1, 2, 3)) < 9\n",
    "        non_final_next_states = next_state_batch[non_final_mask]\n",
    "        \n",
    "        # Computes the state-action values for all actions for all states in the batch\n",
    "        state_action_values = self.policy_net(state_batch)\n",
    "        # For each state, selects only Q(s, a) for the a which was actually chosen\n",
    "        state_action_values = state_action_values.gather(1, action_batch)\n",
    "        \n",
    "        # We now need to compute max_a Q(s', a)\n",
    "        next_state_qvalues = torch.zeros(self.batch_size, device=device)\n",
    "        # We'll set it to zero for final states\n",
    "        # Make sure to use the target network (not the policy) for training stability.\n",
    "        # Note that tensor.max(dim=...) returns a namedtuple (values, indices)\n",
    "        next_state_qvalues[non_final_mask] = self.target_net(non_final_next_states).max(dim=1).values\n",
    "        # Detach the next state values from the gradient graph as it will be used\n",
    "        # as the target in the computation of the loss (We consider it as the \"true qvalue\"\n",
    "        # and hope to converge towards the Bellman equation).\n",
    "        next_state_qvalues = next_state_qvalues.detach()\n",
    "        \n",
    "        # Final objective term\n",
    "        target = reward_batch + self.discount * next_state_qvalues\n",
    "        \n",
    "        # Loss minimization using the optimizer (usual PyTorch training phase)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(state_action_values, target.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # Returns the loss as a float value\n",
    "        return loss.item()\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Udpates the target network by setting its weights\n",
    "        to those of the policy network.\n",
    "        \"\"\"\n",
    "        # We need to make a copy of the policy net's state_dict,\n",
    "        # otherwise we'll keep updating the target net at each iteration\n",
    "        state_dict = self.policy_net.state_dict()\n",
    "        # Calling deepcopy() seems to be the \"best\" way:\n",
    "        # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "        self.target_net.load_state_dict(deepcopy(state_dict))\n",
    "    \n",
    "    def act(self, grid):\n",
    "        \"\"\"\n",
    "        Chooses the action to perform by taking that which returns the\n",
    "        best qvalue, as estimated by the policy network.\n",
    "        Returns the action taken as an integer from 0 to 8.\n",
    "        \"\"\"\n",
    "        # Check whether the epsilon-greedy choice activates\n",
    "        if self.rng_.random() < self.epsilon:\n",
    "            action = torch.tensor([[self.rng_.integers(0, 9)]], device=device)\n",
    "        else:\n",
    "            # We don't want those computations to impact the gradient graph\n",
    "            # somehow\n",
    "            with torch.no_grad():\n",
    "                qvalues = self.policy_net(grid)\n",
    "                # Select the action that has the highest qvalue\n",
    "                # Note that tensor.max(dim=...) returns a namedtuple (values, indices)\n",
    "                action = qvalues.max(dim=1).indices[0]\n",
    "                # The action must have shape (1, 1) so that they can be concatenated\n",
    "                # when sampled from the replay memory\n",
    "                action = action.view(1, 1)\n",
    "        \n",
    "        self.last_state = grid\n",
    "        self.last_action = action\n",
    "        return int(action.item())\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b286b8-cf7f-48f8-8b23-ee1d7528f12c",
   "metadata": {},
   "source": [
    "## Game function for DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1de97-16f4-4347-9ffe-5bbf9b406105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid33_to_332(grid, player, player2value):\n",
    "    \"\"\"\n",
    "    Converts a grid in 3x3 shape whose values are -1, 0 and 1\n",
    "    to the format expected by the DQN player, as a 3x3x2 array.\n",
    "    --player: either 'X' or 'O', which player the DQN agent is.\n",
    "    --player2value: player (X or O) to index (-1 or 1) association,\n",
    "        obtained from the environment.\n",
    "    \"\"\"\n",
    "    grid_332 = np.zeros((3, 3, 2))\n",
    "    # Get the value in the original grid corresponding to the player\n",
    "    # played by the dqn agent:\n",
    "    player_ind = player2value[player]\n",
    "    \n",
    "    grid_332[grid == player_ind, 0] = 1\n",
    "    grid_332[grid == -player_ind, 1] = 1\n",
    "    return grid_332"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e81180-76b9-4494-a691-5018fe6f479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_games_dqn(dqn_player, benchmark_player, nb_games=20000,\n",
    "                   games_between_updates=500,\n",
    "                   turns_swap=\"switch\",\n",
    "                   seed=666):\n",
    "    \"\"\"\n",
    "    Plays a given number of games between two players, and returns the rewards.\n",
    "    --dqn_player: Instance of DQNPlayer to train;\n",
    "    --benchmark_player: Player object implementing act();\n",
    "    --nb_games: How many games should be played;\n",
    "    --games_between_updates: how many games are played between two updates of the agent's\n",
    "        target network.\n",
    "    --turns_swap: str, either \"switch\" to switch turns after every game, or \"random\".\n",
    "    --seed: random seed.\n",
    "    Returns two arrays: rewards, losses\n",
    "    \"\"\"\n",
    "    turns = np.array(['X','O'])\n",
    "    dqn_player.set_player(turns[0])\n",
    "    benchmark_player.set_player(turns[1])\n",
    "    rewards, losses = [], []\n",
    "    env = TictactoeEnv()\n",
    "    \n",
    "    for game in trange(nb_games):\n",
    "        # Sets up the environment for the game\n",
    "        env.reset()\n",
    "        grid, _, _ = env.observe()\n",
    "        # Convert the grid from the env's format to that expected by the agent\n",
    "        grid_tensor = grid33_to_332(grid, dqn_player.player, env.player2value)\n",
    "        grid_tensor = torch.tensor(grid_tensor, device=device).unsqueeze(0).float()\n",
    "        \n",
    "        if turns_swap == \"switch\":\n",
    "            turns = turns[[-1, 0]]\n",
    "        else:\n",
    "            turns = np.random.shuffle(turns)\n",
    "            \n",
    "        dqn_player.set_player(turns[0])\n",
    "        benchmark_player.set_player(turns[1])\n",
    "        \n",
    "        while True:\n",
    "            # Action step\n",
    "            # We now need to account for the case where the agent chooses\n",
    "            # an unavailable position.\n",
    "            if env.current_player == dqn_player.player:\n",
    "                try:\n",
    "                    move = dqn_player.act(grid_tensor)\n",
    "                    grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "                except ValueError:\n",
    "                    # Stop the game and set the reward for the agent to -1\n",
    "                    end = True\n",
    "                    reward = -1\n",
    "            else:\n",
    "                move = benchmark_player.act(grid)\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "                grid_tensor = grid33_to_332(grid, dqn_player.player, env.player2value)\n",
    "                grid_tensor = torch.tensor(grid_tensor, device=device).unsqueeze(0).float()\n",
    "                reward = env.reward(dqn_player.player)\n",
    "\n",
    "                # Learning step\n",
    "                # The DQN agent must have played at least once to start learning\n",
    "                if dqn_player.last_action is not None:\n",
    "                    losses.append(dqn_player.learn(torch.tensor([reward], device=device),\n",
    "                                                   grid_tensor))\n",
    "\n",
    "            if end:\n",
    "                env.reset()\n",
    "                rewards.append(reward)\n",
    "                break\n",
    "            \n",
    "        # Update the agent's target network if required\n",
    "        if game % games_between_updates == 0:\n",
    "            dqn_player.update()\n",
    "    \n",
    "    return np.array(rewards), np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230333e9-692a-45bc-8112-4a0bc2e413d1",
   "metadata": {},
   "source": [
    "### 3.2 Learning from experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d785c-faf2-48f2-8e28-a67f6a77c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_player = DQNPlayer(epsilon=0.05)\n",
    "rewards, losses = play_games_dqn(dqn_player, semi_random_player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd55d4f-975b-4653-9f4e-fff8929cb5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(windowed_avg(rewards))\n",
    "plt.ylim(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52247300-b635-4262-a8df-b703aad442e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
