{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58fa93b-5307-40b0-9b87-1896707c25b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Mini-project 1: Tic-Tac-Toe\n",
    "ClÃ©ment DAUVILLIERS - Florian VINCENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14bd7b-e0ed-415d-a44f-a013a1b5ca3a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31125c7-3f29-43bb-a467-9caaa336a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.typing as npt\n",
    "from tqdm.notebook import trange\n",
    "from typing import List, Tuple, NewType, Union, Callable\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "from player import Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ce705-50db-4f79-9ca4-c1b582865869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250cc033-1d30-4b7f-9f5b-1314532de708",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86870d16-c73e-44e9-97fe-ad9346f7c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid = NewType('Grid', npt.NDArray[np.float64])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b2b5e6-3607-4252-b997-67dcb8d1646a",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20fe206-dcef-43bb-865c-f7d9dfda47c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_avg(arr: npt.ArrayLike, window_len: int=250) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Computes the average over successive windows of an array.\n",
    "    arr must be a 1D array whose length is a multiple of the\n",
    "    window length.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for win_start in range(0, arr.shape[0], window_len):\n",
    "        result.append(np.mean(arr[win_start:win_start + window_len]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1499d2e6-ceb4-4b41-93b3-f7459700b195",
   "metadata": {},
   "source": [
    "# 2. Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb32df3e-82d9-4c5d-8fbe-2b7d7b19ebe0",
   "metadata": {},
   "source": [
    "### QLPlayer class\n",
    "The following class implements the QLearning player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d931d5-1c3d-46e8-b6b1-d15ffcbfb2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLPlayer(Player):\n",
    "    \"\"\"\n",
    "    Implements a player that learns using the QLearning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, player='X',\n",
    "                 lr: float = 0.05,\n",
    "                 discount: float = 0.99,\n",
    "                 epsilon: Callable[[int], float] = lambda _: 0.05,\n",
    "                 qvalues_ref: Union[Grid, None] = None,\n",
    "                 seed: int = 666):\n",
    "        \"\"\"\n",
    "        :param player: either 'X' or 'O';\n",
    "        :param lr: Learning rate;\n",
    "        :param discount: Discount factor, aka gamma;\n",
    "        :param epsilon: function of signature f(iteration) = epsilon, giving\n",
    "            the probability of exploration given the iteration.\n",
    "        :param qvalues_ref: optional reference to a numpy matrix of shape\n",
    "            (number of states, number of actions), which gives the QValues of the agent.\n",
    "            If None, creates a new QValues table initialized to zero.\n",
    "        :param seed: random seed.\n",
    "        \"\"\"\n",
    "        super().__init__(player, epsilon, seed)\n",
    "        self.lr: float = lr\n",
    "        self.discount: float = discount\n",
    "\n",
    "        # Q-values grid\n",
    "        if qvalues_ref is None:\n",
    "            # 3^9 = 19683 states and 9 actions\n",
    "            self.qvalues: Grid = np.zeros((19683, 9))\n",
    "        else:\n",
    "            self.qvalues = qvalues_ref\n",
    "\n",
    "        # Memory\n",
    "        # Starts with None before any action and state are ever seen\n",
    "        self.last_action: Union[None, int] = None\n",
    "        self.last_state: Union[None, Grid] = None\n",
    "\n",
    "    def get_qvalues(self):\n",
    "        \"\"\"\n",
    "        :returns: Table of Qvalues-action association\n",
    "        \"\"\"\n",
    "        return self.qvalues\n",
    "\n",
    "    def act(self,\n",
    "            grid: Grid,\n",
    "            iteration: int = 0) -> int:\n",
    "        \"\"\"\n",
    "        Takes action based on the epsilon-greedy strategy.\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "        :param grid:      current state of the game, as a table\n",
    "        :param iteration: game number, used to modify the exploration\n",
    "                            rate if needed\n",
    "\n",
    "        Returns:\n",
    "        :returns: the chosen action\n",
    "        \"\"\"\n",
    "        # Remember the state for the next learning step\n",
    "        state: int = QLPlayer.state_to_int(grid)\n",
    "        self.last_state: int = state\n",
    "\n",
    "        # Epsilon-greedy choice\n",
    "        chosen_action: int\n",
    "        if self.rng_.random() < self.epsilon(iteration):\n",
    "            chosen_action = self.randomMove(grid)\n",
    "        else:\n",
    "            # Retrieves the list of possible actions and converts them\n",
    "            #     from cell positions to integer indexes.\n",
    "            avail_actions: List[int]\\\n",
    "                = QLPlayer.positions_to_ints(Player.empty(grid))\n",
    "            # Ranks ALL actions according to their Qvalues in the current\n",
    "            #     state.\n",
    "            actions_ranks: npt.NDArray[np.int_]\\\n",
    "                = np.argsort(self.qvalues[state])[::-1]\n",
    "            # Browses all actions in order of their qvalue rank, until\n",
    "            #     finding one that is available.\n",
    "            for action in actions_ranks:\n",
    "                if action in avail_actions:\n",
    "                    # Memorizes the action and the current state for the\n",
    "                    #     learning phase\n",
    "                    chosen_action = int(action)\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(f\"\"\"\n",
    "                        No available action for the current board,\n",
    "                        this is abnormal.\\n\n",
    "                        Available actions:\\n{avail_actions}\\n\n",
    "                        Action ranks computed:\\n{actions_rank}\\n\n",
    "                    \"\"\")\n",
    "\n",
    "        # Remember the action for the learning step\n",
    "        self.last_action = chosen_action\n",
    "        return chosen_action\n",
    "\n",
    "    def learn(self, reward, new_grid, end):\n",
    "        \"\"\"\n",
    "        Updates the Qvalues based on the last (S, A) pair and\n",
    "        the received reward and the new state.\n",
    "        \"\"\"\n",
    "        # If the new_grid is a final state, we can't compute its expected\n",
    "        #     optimal qvalue.\n",
    "        # We instead set it to zero.\n",
    "        if end:\n",
    "            new_state_qval: int = 0\n",
    "        else:\n",
    "            # Computes the optimal Qvalue in the new state max Q(s', a)\n",
    "            new_state: int = QLPlayer.state_to_int(new_grid)\n",
    "            new_state_qval: np.float64 = np.max(self.qvalues[new_state])\n",
    "\n",
    "        # QValue that needs to be updated Q(s, a)\n",
    "        current_qval: np.float64 = self.qvalues[self.last_state,\n",
    "                                                self.last_action]\n",
    "\n",
    "        self.qvalues[self.last_state, self.last_action] += self.lr * (reward + self.discount * new_state_qval - current_qval)\n",
    "\n",
    "    @staticmethod\n",
    "    def position_to_int(position: Tuple[int, int]) -> int:\n",
    "        \"\"\"\n",
    "        (row col) -> row*3 + col\n",
    "        \"\"\"\n",
    "        return position[0] * 3 + position[1]\n",
    "\n",
    "    @staticmethod\n",
    "    def positions_to_ints(positions: List[Tuple[int, int]]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Given a list of cells [(a, b), (c, d), ..],\n",
    "        returns the list of the corresponding indexes.\n",
    "        \"\"\"\n",
    "        return [QLPlayer.position_to_int(cell) for cell in positions]\n",
    "\n",
    "    @staticmethod\n",
    "    def state_to_int(grid: Grid) -> int:\n",
    "        \"\"\"\n",
    "        Converts a grid state to the index of its\n",
    "        row in the lookup table.\n",
    "        \"\"\"\n",
    "        # Converts the grid values from -1, 0, 1 to 0, 1, 2 (a base 3 number)\n",
    "        # Then converts the base 3 number to base 10\n",
    "        return int((np.ravel(grid) + 1) @ np.array([3 ** i for i in range(9)]))\n",
    "\n",
    "    @staticmethod\n",
    "    def int_to_state(state_int: int) -> Grid:\n",
    "        \"\"\"\n",
    "        Converts the index of row in the qvalues table to\n",
    "        its corresponding state.\n",
    "        \"\"\"\n",
    "        # Converts from base 10 to base 3\n",
    "        return np.array([\n",
    "            (state_int % (3 ** (i + 1))) // (3 ** i)\n",
    "            for i in range(9)\n",
    "        ]).reshape((3, 3)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b2b39-d3e1-4de5-abf7-6fd73cd57acf",
   "metadata": {},
   "source": [
    "## 2.1 Learning from experts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96654ae4-a652-4435-945b-5a82a533c250",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea95936-f056-43f1-93af-1072a940bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_games(learning_player: Union[Player, OptimalPlayer],\n",
    "               benchmark_player: Union[Player, OptimalPlayer],\n",
    "               nb_games: int = 20000,\n",
    "               turns_swap: str = \"switch\",\n",
    "               seed: int = 666,\n",
    "               learn: int = 1,\n",
    "               period_Ms: int = 0,\n",
    "               progress=True) -> Tuple[npt.NDArray[np.int_],\n",
    "                                       npt.NDArray[np.float32]]:\n",
    "    \"\"\"\n",
    "    Plays a given number of games between two players, and returns the rewards.\n",
    "    --learning_player: Player object implementing act(), learn(), update();\n",
    "    --benchmark_player: Player object implementing act();\n",
    "    --nb_games: How many games should be played;\n",
    "    --turns_swap: str, either \"switch\" to switch turns after every game,\n",
    "                    or \"random\".\n",
    "    --seed: random seed.\n",
    "    --learn: if 0: nobody learns\n",
    "             if 1: learning player learns when it is his turn\n",
    "             if 2: both players learn while playing\n",
    "\n",
    "    Returns the list of the rewards, in numpy format, and a possibly empty\n",
    "        list of averaged test rewards.\n",
    "    \"\"\"\n",
    "    assert learn < 3\n",
    "\n",
    "    turns = np.array(['X', 'O'])\n",
    "    learning_player.set_player(turns[0])\n",
    "    benchmark_player.set_player(turns[1])\n",
    "    rewards: List[int] = []\n",
    "    env = TictactoeEnv()\n",
    "\n",
    "    # At test time, the swap happens after half of the games have\n",
    "    #     been played.\n",
    "    game_swap = nb_games // 2 if turns_swap == 'half' else None\n",
    "\n",
    "    if period_Ms:\n",
    "        Ms = []\n",
    "    else:\n",
    "        Ms = None\n",
    "\n",
    "    # Decide with the argument progress whether to display the\n",
    "    #     progress bar or not.\n",
    "    if not progress:\n",
    "        game_iterator = range(nb_games)\n",
    "    else:\n",
    "        game_iterator = trange(nb_games)\n",
    "\n",
    "    for game in game_iterator:\n",
    "        # Sets up the environment for the game\n",
    "        env.reset()\n",
    "        grid: Grid = env.observe()[0]\n",
    "        if turns_swap == \"switch\":\n",
    "            learning_player.set_player(j=game)\n",
    "            benchmark_player.set_player(j=game + 1)\n",
    "        elif turns_swap == \"half\":\n",
    "            learning_player.set_player(j=int(game < game_swap))\n",
    "            learning_player.set_player(j=int(game >= game_swap))\n",
    "        else:\n",
    "            turns = np.random.shuffle(turns)\n",
    "            learning_player.set_player(turns[0])\n",
    "            benchmark_player.set_player(turns[1])\n",
    "\n",
    "        while True:\n",
    "            # Action step\n",
    "            if env.current_player == learning_player.player:\n",
    "                move: int = learning_player.act(grid, game)\n",
    "            else:\n",
    "                if learn == 2:\n",
    "                    # If both learn, benchmark_player must be a QLearner\n",
    "                    #     so we can tell it the game number.\n",
    "                    move: int = benchmark_player.act(grid, game)\n",
    "                else:\n",
    "                    move: int = benchmark_player.act(grid)\n",
    "\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "            reward: int = env.reward(learning_player.player)\n",
    "\n",
    "            # Learning step\n",
    "            # The agent learns only after the other has played, as from the\n",
    "            #     point of view of the agent, the next state is not the one\n",
    "            #     right after its move, but the next state in which the agent\n",
    "            #     will need to make a decision.\n",
    "            # If current player == learning player it means the benchmark\n",
    "            #     player just played !\n",
    "            if learn == 2 and\\\n",
    "                    (env.current_player == benchmark_player.player or end):\n",
    "                benchmark_player.learn(reward, grid, end)\n",
    "            if (env.current_player == learning_player.player or end)\\\n",
    "                    and learn > 0:\n",
    "                learning_player.learn(reward, grid, end)\n",
    "\n",
    "            # Computes the reward at the end of each game.\n",
    "            if end:\n",
    "                rewards.append(reward)\n",
    "                break\n",
    "\n",
    "        # Every period_Ms games, the M_opt and M_rand are computed.\n",
    "        if period_Ms * game and game % period_Ms == 0:\n",
    "            # 500 test games for the learning player.\n",
    "            Ms.append(get_Ms(learning_player,\n",
    "                             n_test=500,\n",
    "                             seed=seed))\n",
    "\n",
    "    return np.array(rewards), np.array(Ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d31da3d-659a-4188-a932-96bee00c98e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Ms(learning_player: Union[Player, OptimalPlayer],\n",
    "           n_test: int = 500,\n",
    "           seed: int = 666) -> List[float]:\n",
    "    # First, we play all games facing the optimal player.\n",
    "    # This provides a list of 500 rewards.\n",
    "    rewards_opt: npt.NDArray[np.int_] = \\\n",
    "        play_games(learning_player=learning_player,\n",
    "                   benchmark_player=OptimalPlayer(0.),\n",
    "                   nb_games=n_test,\n",
    "                   turns_swap='half',\n",
    "                   seed=seed,\n",
    "                   learn=0,\n",
    "                   period_Ms=0,\n",
    "                   progress=False)[0]\n",
    "\n",
    "    # We then do the same against the fully-random player.\n",
    "    rewards_rand: npt.NDArray[np.int_] = \\\n",
    "        play_games(learning_player=learning_player,\n",
    "                   benchmark_player=OptimalPlayer(1.),\n",
    "                   nb_games=n_test,\n",
    "                   turns_swap='half',\n",
    "                   seed=seed,\n",
    "                   learn=0,\n",
    "                   period_Ms=0,\n",
    "                   progress=False)[0]\n",
    "\n",
    "    # The M values M_opt and M_rand are then obtained by\n",
    "    #     substracting the number of victories to the\n",
    "    #     number of losses.\n",
    "    # This amounts to taking the mean of the rewards,\n",
    "    #     since we conviniently chose the rewards of the\n",
    "    #     tie games to be 0.\n",
    "    return [rewards_opt.mean(), rewards_rand.mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30906537-06fc-428b-b393-2306165b7736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_epsilon(value):\n",
    "    \"\"\"\n",
    "    Returns a function for epsilon whose value\n",
    "    is independent of the iteration.\n",
    "    \"\"\"\n",
    "    return lambda iteration: value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53ea42-3aad-44b6-abd6-55e7aab286ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "# ##########\n",
    "\n",
    "nb_games = 20000\n",
    "# Defines a QLearning Agent with constant epsilon of 0.05\n",
    "qlplayer = QLPlayer(epsilon=constant_epsilon(0.01), lr=0.05, discount=0.99)\n",
    "semi_random_player = OptimalPlayer(.5)\n",
    "rewards_21 = play_games(qlplayer, semi_random_player, nb_games=nb_games)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a7434-9f97-4867-8cfc-ca93c850d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 250\n",
    "plt.plot(windowed_avg(rewards_21, window_size))\n",
    "plt.ylabel('Average final reward over last 250 games')\n",
    "plt.ylim(-1, 1)\n",
    "plt.xlabel('Games played')\n",
    "xticks = np.arange(nb_games // window_size + 1, step=10)\n",
    "plt.xticks(xticks, xticks * window_size)\n",
    "plt.suptitle(\"QLearning vs Optimal(0.5)\")\n",
    "plt.plot([0, nb_games//window_size], [.5, .5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed8a3c9-f76e-4056-87eb-430a3ce56ba3",
   "metadata": {},
   "source": [
    "### 2.1.1 Decreasing exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b103aa8-a8ad-4619-9799-2dff58a3187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_epsilon(n_star: int,\n",
    "                       e_min: float = .1,\n",
    "                       e_max: float = .8) -> Callable[[int], float]:\n",
    "    \"\"\"\n",
    "    Implements the varying epsilon of equation (1) in the handout.\n",
    "    The function outputs the function $\\\\epsilon(n)$ as a lambda\n",
    "        expression.\n",
    "    epsilon(n) = max( epsilon_min, (1-n/n^*)epsilon_max )\n",
    "    \"\"\"\n",
    "    return lambda n: max(e_min, e_max * (1. - float(n) / float(n_star)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5437bec-51a8-41ba-8a50-eb57639811e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2\n",
    "# ##########\n",
    "\n",
    "nb_games = 20000\n",
    "\n",
    "# We prepare the M values for question 3.\n",
    "list_Ms_plots = []\n",
    "for n_star, c in zip((1, 10, 100, 1000, 10000, 20000),\n",
    "                     ('r', 'b', 'g', 'c', 'k', 'y')):\n",
    "    print(\"Launch games for n*=\", n_star)\n",
    "\n",
    "    # A Q-learner with a decreasing exploration.\n",
    "    # Max exploration at the begining:\n",
    "    #     epsilon_max = 80%\n",
    "    # Min exploration in the end:\n",
    "    #     epsilon_min = 10%\n",
    "    # Number of games before reaching the minimum\n",
    "    #     exploration varies, we call is n_star.\n",
    "    qlplayer = QLPlayer(epsilon=decreasing_epsilon(n_star),\n",
    "                        lr=0.05,\n",
    "                        discount=0.99)\n",
    "    semi_random_player = OptimalPlayer(0.5)\n",
    "\n",
    "    # Collect M values every 250 games, going through 500 games.\n",
    "    rewards_21, Ms = play_games(qlplayer,\n",
    "                                semi_random_player,\n",
    "                                nb_games=nb_games,\n",
    "                                period_Ms=250)\n",
    "    print(\"End of games for n*=\", n_star)\n",
    "    plt.plot(windowed_avg(rewards_21, window_size),\n",
    "             label=f'$n^*=${n_star}', c=c)\n",
    "    list_Ms_plots.append(Ms)\n",
    "plt.ylabel('Average final reward over 250 games for variable $\\\\epsilon(n)$')\n",
    "plt.ylim(-1, 1)\n",
    "plt.xlabel('Games played')\n",
    "xticks = np.arange(nb_games // window_size + 1, step=10)\n",
    "plt.xticks(xticks, xticks * window_size)\n",
    "plt.suptitle(\"QLearning vs Optimal(0.5)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1c670-df36-4ed8-80b7-f7f1d4b6df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3\n",
    "# ##########\n",
    "\n",
    "for Ms, c, n_star in zip(list_Ms_plots,\n",
    "                         ('r', 'b', 'g', 'c', 'k', 'y'),\n",
    "                         (1, 10, 100, 1000, 10000, 20000)):\n",
    "    # Plot the M_opts\n",
    "    plt.plot(np.arange(Ms.shape[0])*250,\n",
    "             Ms[:, 0],\n",
    "             c+'-',\n",
    "             label='$M_{opt}$  $n^*=$' + str(n_star))\n",
    "    # Plot the M_rands\n",
    "    plt.plot(np.arange(Ms.shape[0])*250,\n",
    "             Ms[:, 1],\n",
    "             c+'--',\n",
    "             label='$M_{rand}$ $n^*=$' + str(n_star))\n",
    "plt.legend(loc='upper center', ncol=4, bbox_to_anchor=(0.5, 1.55))\n",
    "plt.ylim([-1, 1])\n",
    "plt.suptitle(\"$M_{opt}$ and $M_{rand}$ for Q learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc61da9-1944-4f55-af64-62376607f671",
   "metadata": {},
   "source": [
    "### 2.1.2 Good experts and bad experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce29f8e5-13bc-4042-ad68-0e63e98c2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4\n",
    "# ##########\n",
    "\n",
    "nb_games = 20000\n",
    "chosen_nstar = 100\n",
    "\n",
    "# We prepare the M values for question 3.\n",
    "list_Ms_plots = []\n",
    "for eps_opt, c in zip(.2 * np.arange(0, 6),\n",
    "                      ('r', 'b', 'g', 'c', 'k', 'y')):\n",
    "    print(f\"Launch games against Opt(epsilon={eps_opt:.1f})\")\n",
    "\n",
    "    # A Q-learner with a decreasing exploration.\n",
    "    # Max exploration at the begining:\n",
    "    #     epsilon_max = 80%\n",
    "    # Min exploration in the end:\n",
    "    #     epsilon_min = 10%\n",
    "    # Number of games before reaching the minimum\n",
    "    #     exploration varies, we call is n_star.\n",
    "    qlplayer = QLPlayer(epsilon=decreasing_epsilon(chosen_nstar),\n",
    "                        lr=0.05,\n",
    "                        discount=0.99)\n",
    "    semi_random_player = OptimalPlayer(eps_opt)\n",
    "\n",
    "    # Collect M values every 250 games, going through 500 games.\n",
    "    rewards_21, Ms = play_games(qlplayer,\n",
    "                                semi_random_player,\n",
    "                                nb_games=nb_games,\n",
    "                                period_Ms=250)\n",
    "    print(f\"End of games for Opt(epsilon={eps_opt:.1f})\")\n",
    "    plt.plot(windowed_avg(rewards_21, window_size),\n",
    "             label=f'$\\\\epsilon=${eps_opt:.1f}', c=c)\n",
    "    list_Ms_plots.append(Ms)\n",
    "plt.ylabel('Average final reward over 250 games for variable $\\\\epsilon_{opt}$')\n",
    "plt.ylim(-1, 1)\n",
    "plt.xlabel('Games played')\n",
    "xticks = np.arange(nb_games // window_size + 1, step=10)\n",
    "plt.xticks(xticks, xticks * window_size)\n",
    "plt.suptitle(\"QLearning vs Optimal($\\\\epsilon_{opt}$)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7235bd0-a478-489c-99eb-707e9dad1220",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Ms, c, eps in zip(list_Ms_plots,\n",
    "                      ('r', 'b', 'g', 'c', 'k', 'y'),\n",
    "                      .1 * np.arange(0, 6)):\n",
    "    legd = f\"{eps:.1f}\"\n",
    "\n",
    "    # Plot the M_opts\n",
    "    plt.plot(np.arange(Ms.shape[0])*250,\n",
    "             Ms[:, 0],\n",
    "             c+'-',\n",
    "             label='$M_{opt}$  vs $\\\\epsilon_{opt}=$' + legd)\n",
    "    # Plot the M_rands\n",
    "    plt.plot(np.arange(Ms.shape[0])*250,\n",
    "             Ms[:, 1],\n",
    "             c+'--',\n",
    "             label='$M_{rand}$ vs $\\\\epsilon_{opt}=$' + legd)\n",
    "plt.legend(loc='upper center', ncol=4, bbox_to_anchor=(0.5, 1.55))\n",
    "plt.ylim([-1, 1])\n",
    "plt.suptitle(\"\"\"\n",
    "    $M_{opt}$ and $M_{rand}$ for Q learning, for various $\\\\epsilon_{opt}$\n",
    "    \"\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f56127-d3ed-4e9e-bc64-13f7dcf31394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5\n",
    "# ##########\n",
    "\n",
    "all_Ms = np.stack(list_Ms_plots, axis=0)\n",
    "print(f\"Maximum value of M_opt  obtained: {np.max(all_Ms[:, :, 0])}\")\n",
    "print(f\"Maximum value of M_rand obtained: {np.max(all_Ms[:, :, 1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aae948-6cde-41dc-bb52-537001d98ec9",
   "metadata": {},
   "source": [
    "## 2.2 Learning by self practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a359b23-739c-4562-8dd1-e544b7a7dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_alone(epsilon: Callable[[int], float],\n",
    "               nb_games: int = 20000\n",
    "               ) -> Tuple[npt.NDArray[np.int_],\n",
    "                          npt.NDArray[np.float32],\n",
    "                          QLPlayer]:\n",
    "    r\"\"\"\n",
    "    Plays nb_games with a Q-learner against itself.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    :param epsilon: function taking the game number as an argument:\n",
    "        .. math::\n",
    "            \\epsilon(n) = \\max\\left(\\epsilon_{min},\n",
    "                \\epsilon_{max}(1-\\frac{n}{n^*})\\right)\n",
    "\n",
    "    :param nb_games: number of games to play\n",
    "\n",
    "    :returns: array of rewards and stacked array of M_opts and M_rands\n",
    "    \"\"\"\n",
    "    shared_grid: Grid = np.zeros((19683, 9))\n",
    "\n",
    "    # Defines a QLearning Agent sharing a Q grid with its alter ego.\n",
    "    qlplayer = QLPlayer(epsilon=epsilon,\n",
    "                        lr=0.05,\n",
    "                        discount=0.99,\n",
    "                        qvalues_ref=shared_grid)\n",
    "\n",
    "    # Play all games using the same reference for the player,\n",
    "    #     and a shared reference for the Q values.\n",
    "    rewards, Ms = play_games(qlplayer,\n",
    "                             qlplayer,\n",
    "                             nb_games=nb_games,\n",
    "                             period_Ms=250,\n",
    "                             learn=2)\n",
    "\n",
    "    return rewards, Ms, qlplayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe8853-43b4-4584-ab77-6e8b182e3930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 7\n",
    "# ##########\n",
    "\n",
    "list_Ms_plots = []\n",
    "for eps, c in zip(.1 * np.arange(0, 6),\n",
    "                  ('r', 'b', 'g', 'c', 'k', 'y')):\n",
    "    print(f\"Launch self-games # epsilon={eps:.1f}\")\n",
    "    rewards, Ms, _ = play_alone(constant_epsilon(eps))\n",
    "    list_Ms_plots.append(Ms)\n",
    "    plt.plot(windowed_avg(rewards, window_size),\n",
    "             label=f'$\\\\epsilon=${eps:.1f}', c=c)\n",
    "    print(f\"End of self-games # epsilon={eps:.1f}\")\n",
    "plt.ylabel('Average final reward over 250 games')\n",
    "plt.ylim(-1, 1)\n",
    "plt.xlabel('Games played')\n",
    "xticks = np.arange(nb_games // window_size + 1, step=10)\n",
    "plt.xticks(xticks, xticks * window_size)\n",
    "plt.suptitle(r\"\"\"QLearning vs oneself for variable exploration\n",
    "    rates $\\epsilon$\"\"\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a4ede7-b950-4453-af2e-3a488575504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Ms, c, eps in zip(list_Ms_plots,\n",
    "                         ('r', 'b', 'g', 'c', 'k', 'y'),\n",
    "                         .1 * np.arange(0, 6)):\n",
    "    legd = f\"{eps:.1f}\"\n",
    "\n",
    "    # Plot the M_opts\n",
    "    plt.plot(np.arange(Ms.shape[0])*250,\n",
    "             Ms[:, 0],\n",
    "             c+'-',\n",
    "             label='$M_{opt}$  $\\\\epsilon=$' + legd)\n",
    "    # Plot the M_rands\n",
    "    plt.plot(np.arange(Ms.shape[0])*250,\n",
    "             Ms[:, 1],\n",
    "             c+'--',\n",
    "             label='$M_{rand}$ $\\\\epsilon=$' + legd)\n",
    "plt.legend(loc='upper center', ncol=4, bbox_to_anchor=(0.5, 1.55))\n",
    "plt.ylim([-1, 1])\n",
    "plt.suptitle(\"$M_{opt}$ and $M_{rand}$ for self-taught Q learning\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a91977-1674-41db-854a-7e2ad4e3dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 8\n",
    "# ##########\n",
    "\n",
    "list_Ms_plots = []\n",
    "for n_star, c in zip((1, 10, 100, 1000, 10000, 20000),\n",
    "                     ('r', 'b', 'g', 'c', 'k', 'y')):\n",
    "    print(f\"Launch self-games # n*={n_star:.1f}\")\n",
    "    rewards, Ms, _ = play_alone(epsilon=decreasing_epsilon(n_star=n_star))\n",
    "    list_Ms_plots.append(Ms)\n",
    "    plt.plot(windowed_avg(rewards, window_size),\n",
    "             label=f'$n^*=${n_star:.1f}', c=c)\n",
    "    print(f\"End of self-games # n*={n_star:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f1392d-45b4-47de-9a6c-d023d56e1fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Ms, c, n_star in zip(list_Ms_plots,\n",
    "                         ('r', 'b', 'g', 'c', 'k', 'y'),\n",
    "                         (1, 10, 100, 1000, 10000, 20000)):\n",
    "    legd = f\"{n_star:.1f}\"\n",
    "\n",
    "    # Plot the M_opts\n",
    "    plt.plot(np.arange(Ms.shape[0])*250,\n",
    "             Ms[:, 0],\n",
    "             c+'-',\n",
    "             label='$M_{opt}$  $n^*=$' + legd)\n",
    "    # Plot the M_rands\n",
    "    plt.plot(np.arange(Ms.shape[0])*250,\n",
    "             Ms[:, 1],\n",
    "             c+'--',\n",
    "             label='$M_{rand}$ $n^*=$' + legd)\n",
    "plt.legend(loc='upper center', ncol=4, bbox_to_anchor=(0.5, 1.55))\n",
    "plt.ylim([-1, 1])\n",
    "plt.suptitle(\"$M_{opt}$ and $M_{rand}$ for self-taught Q learning\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2823ea9c-ad3f-4dfb-825a-923deb959927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 9\n",
    "# ##########\n",
    "\n",
    "all_Ms = np.stack(list_Ms_plots, axis=0)\n",
    "print(f\"Maximum value of M_opt  obtained: {np.max(all_Ms[:, :, 0])}\")\n",
    "print(f\"Maximum value of M_rand obtained: {np.max(all_Ms[:, :, 1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fcbff1-f445-4cc6-8cbe-e04aeb18c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 10\n",
    "# ###########\n",
    "\n",
    "def get_qvals_for_state(study_state: Grid,\n",
    "                        player: QLPlayer) -> Grid:\n",
    "    state: int = QLPlayer.state_to_int(study_state)\n",
    "    return player.get_qvalues()[state, :].reshape(3, 3)\n",
    "\n",
    "\n",
    "def plot_three_choices(player: QLPlayer) -> None:\n",
    "    correspondance: dict = {0: '-', 1: 'X', -1: 'O'}\n",
    "\n",
    "    fig, axs = plt.subplots(3, 2)\n",
    "    rendering_env = TictactoeEnv()\n",
    "\n",
    "    desc1 = '''Game begins, I am X, what do I do?'''\n",
    "    CHOICE1: Grid = np.zeros((3, 3), dtype=np.float64)\n",
    "\n",
    "    desc2 = '''X started and put on the corner\n",
    "        (it seems like it is not Opt(1)),\\n\n",
    "         I am O, what do I do?'''\n",
    "    CHOICE2: Grid = np.array([[0., 0., 0.],\n",
    "                              [0., 1., 0.],\n",
    "                              [0., 0., 0.]], dtype=np.float64)\n",
    "\n",
    "    desc3 = '''X started, I am 0 and I derped on turn 2\n",
    "        by playing left side.\\n\n",
    "        So did X by playing again in the corner.\\n\n",
    "        Should I play in the middle?'''\n",
    "    CHOICE3: Grid = np.array([[1., 0., 0.],\n",
    "                              [-1., 0., 0.],\n",
    "                              [0., 0., 1.]], dtype=np.float64)\n",
    "\n",
    "    for k, desc, choice in zip(range(3),\n",
    "                               (desc1, desc2, desc3),\n",
    "                               (CHOICE1, CHOICE2, CHOICE3)):\n",
    "        print(\"Scenario \"+str(k+1)+\":\")\n",
    "        print(desc)\n",
    "        qvals: Grid = get_qvals_for_state(choice, player)\n",
    "        rendering_env.grid = choice\n",
    "        rendering_env.render()\n",
    "\n",
    "        ax = axs[k, 0]\n",
    "        ax.imshow(qvals, cmap='cool')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                ax.text(j, i, correspondance[choice[i, j]],\n",
    "                        ha=\"center\", va=\"center\", color='black')\n",
    "\n",
    "        ax = axs[k, 1]\n",
    "        ax.text(.5, .5, desc, ha=\"center\", va=\"center\")\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7dd625-4c14-4656-a69d-fcfa8d41a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, good_player = play_alone(epsilon=decreasing_epsilon(n_star=int(1e5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4970bbf0-53e5-4cb7-8f6a-16e6d453ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_three_choices(good_player)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5242a9-7f44-4147-80de-4bd8ca20a9a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Deep Q-Learning\n",
    "All of the following implementations will be based on the PyTorch RL Tutorial:  \n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.htmlhttps://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7abb642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqnplayer import DQNPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79746898-a689-4fd0-abee-6c7897c628cf",
   "metadata": {},
   "source": [
    "We'll try to work on a GPU if one is available to PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27fc3e-b8aa-4a4c-b391-7ebecf264f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b286b8-cf7f-48f8-8b23-ee1d7528f12c",
   "metadata": {},
   "source": [
    "## Game function for DQN\n",
    "We'll now write the function that wraps the whole training loop.  \n",
    "First, we need two functions to manipulate states and make them coherent with the format expected by the DQN agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1de97-16f4-4347-9ffe-5bbf9b406105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid33_to_332(grid, player, player2value):\n",
    "    \"\"\"\n",
    "    Converts a grid in 3x3 shape whose values are -1, 0 and 1\n",
    "    to the format expected by the DQN player, as a 3x3x2 array.\n",
    "    --player: either 'X' or 'O', which player the DQN agent is.\n",
    "    --player2value: player (X or O) to index (-1 or 1) association,\n",
    "        obtained from the environment.\n",
    "    \"\"\"\n",
    "    grid_332 = np.zeros((3, 3, 2))\n",
    "    # Get the value in the original grid corresponding to the player\n",
    "    # played by the dqn agent:\n",
    "    player_ind = player2value[player]\n",
    "\n",
    "    grid_332[grid == player_ind, 0] = 1\n",
    "    grid_332[grid == -player_ind, 1] = 1\n",
    "    return grid_332\n",
    "\n",
    "\n",
    "def grid_to_tensor(grid, env):\n",
    "    \"\"\"\n",
    "    Converts the numpy array grid returned by the environment to a torch\n",
    "    tensor in the format expected by the DQN agent.\n",
    "    \"\"\"\n",
    "    grid_tensor = grid33_to_332(grid, dqn_player.player, env.player2value)\n",
    "    grid_tensor = torch.tensor(grid_tensor, device=device).unsqueeze(0).float()\n",
    "    return grid_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c1e36-b6cc-482a-9adb-2c67aa0ca7ac",
   "metadata": {},
   "source": [
    "We can now write the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e81180-76b9-4494-a691-5018fe6f479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_games_dqn(dqn_player, benchmark_player, nb_games=20000,\n",
    "                   games_between_updates=500,\n",
    "                   seed=666):\n",
    "    \"\"\"\n",
    "    Plays a given number of games between two players, and returns the rewards.\n",
    "    --dqn_player: Instance of DQNPlayer to train;\n",
    "    --benchmark_player: Player object implementing act();\n",
    "    --nb_games: How many games should be played;\n",
    "    --games_between_updates: how many games are played between two updates of the agent's\n",
    "        target network.\n",
    "    --seed: random seed.\n",
    "    Returns two arrays: rewards, losses\n",
    "    \"\"\"\n",
    "    rewards, losses = [], []\n",
    "    env = TictactoeEnv()\n",
    "\n",
    "    for game in trange(nb_games):\n",
    "        # Sets up the environment for the game\n",
    "        env.reset()\n",
    "        grid, _, _ = env.observe()\n",
    "\n",
    "        # Switch turns\n",
    "        dqn_player.set_player(j=game)\n",
    "        benchmark_player.set_player(j=game + 1)\n",
    "\n",
    "        # Convert the grid from the env's format to that expected by the agent\n",
    "        grid_tensor = grid_to_tensor(grid, env)\n",
    "\n",
    "        game_losses = []\n",
    "\n",
    "        while True:\n",
    "            # Action step\n",
    "            # We now need to account for the case where the agent chooses\n",
    "            # an unavailable position.\n",
    "            try:\n",
    "                if env.current_player == dqn_player.player:\n",
    "                    move = dqn_player.act(grid_tensor)\n",
    "                else:\n",
    "                    move = benchmark_player.act(grid)\n",
    "\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "                grid_tensor = grid_to_tensor(grid, env)\n",
    "                reward = env.reward(dqn_player.player)\n",
    "\n",
    "            except ValueError:\n",
    "                # Being here means the agent chose an impossible action\n",
    "                # Stop the game and set the reward for the agent to -1\n",
    "                end = True\n",
    "                reward = -1\n",
    "\n",
    "            # Learning step\n",
    "            # The DQN agent must have played at least once to start learning\n",
    "            if env.current_player == dqn_player.player or end:\n",
    "                # For the DQN agent, final states will be represented as None\n",
    "                if end:\n",
    "                    grid_tensor = None\n",
    "                game_losses.append(dqn_player.learn(torch.tensor([reward], device=device),\n",
    "                                               grid_tensor))\n",
    "\n",
    "            if end:\n",
    "                losses.append(np.mean(game_losses))\n",
    "                rewards.append(reward)\n",
    "                break\n",
    "\n",
    "        # Update the agent's target network if required\n",
    "        if game % games_between_updates == 0:\n",
    "            dqn_player.update()\n",
    "\n",
    "    return np.array(rewards), np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230333e9-692a-45bc-8112-4a0bc2e413d1",
   "metadata": {},
   "source": [
    "## 3.2 Learning from experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d785c-faf2-48f2-8e28-a67f6a77c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_opt_player = OptimalPlayer(0.5)\n",
    "dqn_player = DQNPlayer(device, epsilon=0.01)\n",
    "\n",
    "# Trains the DQN Agent by playing a semi-optimal player\n",
    "nb_games = 20000\n",
    "rewards, losses = play_games_dqn(dqn_player, semi_opt_player, nb_games=nb_games)\n",
    "\n",
    "# Plots the window-averaged rewards\n",
    "window_size = 250\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1)\n",
    "ax[0].plot(windowed_avg(rewards, window_size))\n",
    "ax[0].set_ylabel('Average final reward over 250 games')\n",
    "ax[0].set_ylim(-1, 1)\n",
    "xticks = np.arange(nb_games // window_size + 1, step=10)\n",
    "labels = [str(k) for k in xticks * window_size]\n",
    "ax[0].set_xticks(xticks)\n",
    "ax[0].set_xticklabels(labels)\n",
    "\n",
    "# Plots average game losses\n",
    "ax[1].plot(windowed_avg(losses, window_size))\n",
    "ax[1].set_ylabel('Average loss over 250 games')\n",
    "ax[1].set_xticks(xticks)\n",
    "ax[1].set_xticklabels(labels)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
