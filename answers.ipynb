{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58fa93b-5307-40b0-9b87-1896707c25b9",
   "metadata": {},
   "source": [
    "# Mini-project 1: Tic-Tac-Toe\n",
    "ClÃ©ment DAUVILLIERS - Florian VINCENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14bd7b-e0ed-415d-a44f-a013a1b5ca3a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250cc033-1d30-4b7f-9f5b-1314532de708",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31125c7-3f29-43bb-a467-9caaa336a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange\n",
    "from math import log\n",
    "from copy import deepcopy\n",
    "from queue import deque\n",
    "from random import sample\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "import numpy.typing as npt\n",
    "from typing import List, Tuple, NewType, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86870d16-c73e-44e9-97fe-ad9346f7c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid = NewType('Grid', npt.NDArray[np.float64])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b2b5e6-3607-4252-b997-67dcb8d1646a",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20fe206-dcef-43bb-865c-f7d9dfda47c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_avg(arr: npt.ArrayLike, window_len: int=250) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Computes the average over successive windows of an array.\n",
    "    arr must be a 1D array whose length is a multiple of the\n",
    "    window length.\n",
    "    \"\"\"\n",
    "    return arr.reshape((window_len, -1)).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c32c475-22d4-4e03-9dac-d32b5adef9eb",
   "metadata": {},
   "source": [
    "## Player class\n",
    "The following class will be used as base for the QLearning and DQN player classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b11f77-858b-4ce3-a796-c5612e69154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    \"\"\"\n",
    "    Base class for both types of players (QLearning, DQN).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 player: str = 'X',\n",
    "                 epsilon: float = 0.05,\n",
    "                 seed: int = 666):\n",
    "        self.player: str = player\n",
    "        self.epsilon: float = epsilon\n",
    "        \n",
    "        # RNG for the epsilon-gredy policy\n",
    "        self.rng_ = np.random.default_rng(seed=seed)\n",
    "    \n",
    "    def act(self, grid: Grid) -> int:\n",
    "        \"\"\"\n",
    "        Selects an action to perform based on the current\n",
    "        grid state and the player's policy.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Call from abstract class\")\n",
    "        \n",
    "    def set_player(self, player: str) -> None:\n",
    "        self.player = player\n",
    "    \n",
    "    @staticmethod\n",
    "    def empty(grid: Grid) -> List[Tuple[int, int]]:\n",
    "        '''return all empty positions'''\n",
    "        avail = []\n",
    "        for i in range(9):\n",
    "            pos = (int(i/3), i % 3)\n",
    "            if grid[pos] == 0:\n",
    "                avail.append(pos)\n",
    "        return avail\n",
    "        \n",
    "    def randomMove(self, grid: Grid) -> Tuple[int, int]:\n",
    "        \"\"\" Chose a random move from the available options. \"\"\"\n",
    "        avail = self.empty(grid)\n",
    "\n",
    "        return avail[self.rng_.integers(0, len(avail))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1499d2e6-ceb4-4b41-93b3-f7459700b195",
   "metadata": {},
   "source": [
    "# 2. Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb32df3e-82d9-4c5d-8fbe-2b7d7b19ebe0",
   "metadata": {},
   "source": [
    "### QLPlayer class\n",
    "The following class implements the QLearning player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d931d5-1c3d-46e8-b6b1-d15ffcbfb2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLPlayer(Player):\n",
    "    \"\"\"\n",
    "    Implements a player that learns using the QLearning algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, player='X',\n",
    "                 lr: float = 0.05,\n",
    "                 discount: float = 0.99,\n",
    "                 epsilon: float = 0.05,\n",
    "                 seed: int = 666):\n",
    "        super().__init__(player, epsilon, seed)\n",
    "        self.lr: float = lr\n",
    "        self.discount: float = discount\n",
    "        self.epsilon: float = epsilon\n",
    "        \n",
    "        # Q-values grid\n",
    "        # 3^9 = 19683 states and 9 actions\n",
    "        self.qvalues: Grid = np.zeros((19683, 9))\n",
    "        \n",
    "        # Memory\n",
    "        self.last_action: Union[None, int] = None\n",
    "        self.last_state: Union[None, Grid] = None\n",
    "        \n",
    "    def act(self, grid):\n",
    "        # Inverts the grid if required\n",
    "        grid = self.invert_grid(grid)\n",
    "        \n",
    "        # Epsilon-greedy choice\n",
    "        if self.rng_.random() < self.epsilon:\n",
    "            return self.randomMove(grid)\n",
    "        # Retrieves the list of possible actions and converts them\n",
    "        # from cell positions to integer indexes\n",
    "        avail_actions: List[int] = QLPlayer.positions_to_ints(Player.empty(grid))\n",
    "        # Ranks ALL actions according to their Qvalues in the current\n",
    "        # state\n",
    "        state: int = QLPlayer.state_to_int(grid)\n",
    "        actions_ranks: npt.NDArray = np.argsort(self.qvalues[state])[::-1]\n",
    "        # Browses all actions in order of their qvalue rank, until\n",
    "        # finding one that is available\n",
    "        for action in actions_ranks:\n",
    "            if action in avail_actions:\n",
    "                # Memorizes the action and the current state for the learning\n",
    "                # phase\n",
    "                self.last_action, self.last_state = action, state\n",
    "                return int(action)\n",
    "    \n",
    "    def learn(self, \n",
    "              reward: int,\n",
    "              new_grid: Grid,\n",
    "              end: int) -> None:\n",
    "        \"\"\"\n",
    "        Updates the Qvalues based on the last (S, A) pair and\n",
    "        the received reward and the new state.\n",
    "        \"\"\"\n",
    "        # If the new_grid is a final state, we can't compute its expected optimal\n",
    "        # qvalue. We instead set it to zero.\n",
    "        if end:\n",
    "            new_state_qval: int = 0\n",
    "        else:\n",
    "            # Inverts the grid if required (so that ones corresponding\n",
    "            # to THIS player's chesses).\n",
    "            new_grid: Grid = self.invert_grid(new_grid)\n",
    "\n",
    "            # Computes the optimal Qvalue in the new state max Q(s', a)\n",
    "            new_state: int = QLPlayer.state_to_int(new_grid)\n",
    "            new_state_qval: np.float64 = np.max(self.qvalues[new_state])\n",
    "        \n",
    "        # QValue that needs to be updated Q(s, a)\n",
    "        current_qval: np.float64 = self.qvalues[self.last_state, self.last_action]\n",
    "        \n",
    "        self.qvalues[self.last_state, self.last_action] += self.lr * (reward + self.discount * new_state_qval - current_qval)\n",
    "    \n",
    "    def invert_grid(self, grid: Grid) -> Grid:\n",
    "        \"\"\"\n",
    "        Returns a version of the grid in which the ones correspond to this player's\n",
    "        chesses.\n",
    "        \"\"\"\n",
    "        # If we play with the 'O', then the -1 in the grid are actually our pieces\n",
    "        return grid if self.player == 'X' else -grid\n",
    "    \n",
    "    @staticmethod\n",
    "    def position_to_int(position: Tuple[int, int]) -> int:\n",
    "        \"\"\"\n",
    "        (row col) -> row*3 + col\n",
    "        \"\"\"\n",
    "        return position[0] * 3 + position[1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def positions_to_ints(positions: List[Tuple[int, int]]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Given a list of cells [(a, b), (c, d), ..],\n",
    "        returns the list of the corresponding indexes.\n",
    "        \"\"\"\n",
    "        return [QLPlayer.position_to_int(cell) for cell in positions]\n",
    "\n",
    "    @staticmethod\n",
    "    def state_to_int(grid: Grid) -> int:\n",
    "        \"\"\"\n",
    "        Converts a grid state to the index of its\n",
    "        row in the lookup table.\n",
    "        \"\"\"\n",
    "        # Converts the grid values from -1, 0, 1 to 0, 1, 2 (a base 3 number)\n",
    "        # Then converts the base 3 number to base 10\n",
    "        return int((np.ravel(grid) + 1) @ np.array([3 ** i for i in range(9)]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def int_to_state(state_int: int) -> Grid:\n",
    "        \"\"\"\n",
    "        Converts the index of row in the qvalues table to\n",
    "        its corresponding state.\n",
    "        \"\"\"\n",
    "        # Converts from base 10 to base 3\n",
    "        return np.array([\n",
    "            (state_int % (3 ** (i + 1))) // (3 ** i)\n",
    "            for i in range(9)\n",
    "        ]).reshape((3, 3)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b2b39-d3e1-4de5-abf7-6fd73cd57acf",
   "metadata": {},
   "source": [
    "## 2.1 Learning from experts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96654ae4-a652-4435-945b-5a82a533c250",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea95936-f056-43f1-93af-1072a940bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_games(learning_player: Union[Player, OptimalPlayer],\n",
    "               benchmark_player: Union[Player, OptimalPlayer],\n",
    "               nb_games: int = 20000,\n",
    "               turns_swap: str = \"switch\",\n",
    "               seed: int = 666,\n",
    "               learn: bool = True) -> npt.NDArray[np.int_]:\n",
    "    \"\"\"\n",
    "    Plays a given number of games between two players, and returns the rewards.\n",
    "    --learning_player: Player object implementing act(), learn(), update();\n",
    "    --benchmark_player: Player object implementing act();\n",
    "    --nb_games: How many games should be played;\n",
    "    --turns_swap: str, either \"switch\" to switch turns after every game, or \"random\".\n",
    "    --seed: random seed.\n",
    "    \"\"\"\n",
    "    turns: npt.NDArray[np.string_] = np.array(['X','O'])\n",
    "    learning_player.set_player(turns[0])\n",
    "    benchmark_player.set_player(turns[1])\n",
    "    rewards: List[int] = []\n",
    "    env = TictactoeEnv()\n",
    "    \n",
    "    for game in trange(nb_games):\n",
    "        # Sets up the environment for the game\n",
    "        env.reset()\n",
    "        grid: Grid = env.observe()[0]\n",
    "        if turns_swap == \"switch\":\n",
    "            turns = turns[[-1, 0]]\n",
    "        else:\n",
    "            turns = np.random.shuffle(turns)\n",
    "        learning_player.set_player(turns[0])\n",
    "        benchmark_player.set_player(turns[1])\n",
    "        \n",
    "        while True:\n",
    "            # Action step\n",
    "            if env.current_player == learning_player.player:\n",
    "                move: int = learning_player.act(grid)\n",
    "            else:\n",
    "                move: int = benchmark_player.act(grid)\n",
    "\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "            reward: int = env.reward(learning_player.player)\n",
    "\n",
    "            # Learning step\n",
    "            # The agent learns only after the other has played, as from the\n",
    "            # point of view of the agent, the next state is not the one right after\n",
    "            # its move, but the next state in which the agent will need to make a decision.\n",
    "            if (env.current_player == benchmark_player.player or end) and learn:\n",
    "                learning_player.learn(reward, grid, end)\n",
    "\n",
    "            if end:\n",
    "                env.reset()\n",
    "                rewards.append(reward)\n",
    "                break\n",
    "    \n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e865c195-e3d9-4a6a-9603-c233a9471915",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlplayer = QLPlayer(epsilon=0.1)\n",
    "semi_random_player = OptimalPlayer(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53ea42-3aad-44b6-abd6-55e7aab286ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_21 = play_games(qlplayer, semi_random_player, nb_games=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a7434-9f97-4867-8cfc-ca93c850d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(windowed_avg(rewards_21))\n",
    "plt.ylim(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5242a9-7f44-4147-80de-4bd8ca20a9a0",
   "metadata": {},
   "source": [
    "# 3. Deep Q-Learning\n",
    "All of the following implementations will be based on the PyTorch RL Tutorial:  \n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.htmlhttps://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ce705-50db-4f79-9ca4-c1b582865869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79746898-a689-4fd0-abee-6c7897c628cf",
   "metadata": {},
   "source": [
    "We'll try to work on a GPU if one is available to PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27fc3e-b8aa-4a4c-b391-7ebecf264f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315c8eb-2938-48a3-8959-507049afcd7c",
   "metadata": {},
   "source": [
    "## Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f38b6-dda0-4e27-ae44-498ac117aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    state (3,3,2)->flatten->(18,) nn input\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.input = nn.Linear(18, 128)\n",
    "        self.hidden1 = nn.Linear(128, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.output = nn.Linear(128, 9)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        # Flattens x to make sure it can be passed to the linear layers\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        # No activation is a linear activation\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2681db-9713-440a-bbea-73a865d205d9",
   "metadata": {},
   "source": [
    "## Replay memory\n",
    "This class is directly taken from the PyTorch DQN tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf7201-8a61-456b-a638-eff02c09e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5455aa26-ee1a-4d0b-b329-284c6b75278f",
   "metadata": {},
   "source": [
    "## DQNPlayer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f19284-361f-400f-9f62-66dedd5e9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNPlayer(Player):\n",
    "    \"\"\"\n",
    "    Implements a type of Player that uses Deep Q-Learning\n",
    "    to learn the tictactoe strategy.\n",
    "    \"\"\"\n",
    "    def __init__(self, player='X', lr=5e-4, discount=0.99, epsilon=0.05, batch_size=64,\n",
    "                  seed=666):\n",
    "        super().__init__(player, epsilon, seed)\n",
    "        self.lr = lr\n",
    "        self.discount = discount\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.last_action, self.last_state = None, None\n",
    "        \n",
    "        # Neural networks\n",
    "        self.policy_net = DQN().to(device)\n",
    "        self.target_net = DQN().to(device)\n",
    "        \n",
    "        self.memory = ReplayMemory(10000)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Huber loss\n",
    "        self.criterion = nn.HuberLoss()\n",
    "        # Adam optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
    "    \n",
    "    def learn(self, reward, grid):\n",
    "        \"\"\"\n",
    "        Stores the last (S, A, NS, R) tuple into the replay memory,\n",
    "        and trains the policy network using a sample from the replay memory.\n",
    "        --reward: float, end game reward\n",
    "        --grid: (3, 3, 2) array representing the current state.\n",
    "        Returns the value of the Huber loss.\n",
    "        \"\"\"\n",
    "        # Push the last experience into the replay memory\n",
    "        self.memory.push(self.last_state, self.last_action, grid, reward)\n",
    "        \n",
    "        # We don't start learning before the replay memory is large enough to\n",
    "        # return at least a full batch\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        ## Policy network training ============================================\n",
    "        # Taken from the Pytorch RL tutorial\n",
    "        # First sample a batch of Transition objects\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # Then creates a single Transition obj whose elements are arrays\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        next_state_batch = torch.cat(batch.next_state)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        # We need to know where in the batch the next state is final\n",
    "        # as we can't compute max Q(ns, .) for those.\n",
    "        # The final states are characterized by having the sum of the grid equal to 9\n",
    "        # (1 for each cell).\n",
    "        non_final_mask = next_state_batch.sum((1, 2, 3)) < 9\n",
    "        non_final_next_states = next_state_batch[non_final_mask]\n",
    "        \n",
    "        # Computes the state-action values for all actions for all states in the batch\n",
    "        state_action_values = self.policy_net(state_batch)\n",
    "        # For each state, selects only Q(s, a) for the a which was actually chosen\n",
    "        state_action_values = state_action_values.gather(1, action_batch)\n",
    "        \n",
    "        # We now need to compute max_a Q(s', a)\n",
    "        next_state_qvalues = torch.zeros(self.batch_size, device=device)\n",
    "        # We'll set it to zero for final states\n",
    "        # Make sure to use the target network (not the policy) for training stability.\n",
    "        # Note that tensor.max(dim=...) returns a namedtuple (values, indices)\n",
    "        next_state_qvalues[non_final_mask] = self.target_net(non_final_next_states).max(dim=1).values\n",
    "        # Detach the next state values from the gradient graph as it will be used\n",
    "        # as the target in the computation of the loss (We consider it as the \"true qvalue\"\n",
    "        # and hope to converge towards the Bellman equation).\n",
    "        next_state_qvalues = next_state_qvalues.detach()\n",
    "        \n",
    "        # Final objective term\n",
    "        target = reward_batch + self.discount * next_state_qvalues\n",
    "        \n",
    "        # Loss minimization using the optimizer (usual PyTorch training phase)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(state_action_values, target.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # Returns the loss as a float value\n",
    "        return loss.item()\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Udpates the target network by setting its weights\n",
    "        to those of the policy network.\n",
    "        \"\"\"\n",
    "        # We need to make a copy of the policy net's state_dict,\n",
    "        # otherwise we'll keep updating the target net at each iteration\n",
    "        state_dict = self.policy_net.state_dict()\n",
    "        # Calling deepcopy() seems to be the \"best\" way:\n",
    "        # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "        self.target_net.load_state_dict(deepcopy(state_dict))\n",
    "    \n",
    "    def act(self, grid):\n",
    "        \"\"\"\n",
    "        Chooses the action to perform by taking that which returns the\n",
    "        best qvalue, as estimated by the policy network.\n",
    "        Returns the action taken as an integer from 0 to 8.\n",
    "        \"\"\"\n",
    "        # Check whether the epsilon-greedy choice activates\n",
    "        if self.rng_.random() < self.epsilon:\n",
    "            action = torch.tensor([[self.rng_.integers(0, 9)]], device=device)\n",
    "        else:\n",
    "            # We don't want those computations to impact the gradient graph\n",
    "            # somehow\n",
    "            with torch.no_grad():\n",
    "                qvalues = self.policy_net(grid)\n",
    "                # Select the action that has the highest qvalue\n",
    "                # Note that tensor.max(dim=...) returns a namedtuple (values, indices)\n",
    "                action = qvalues.max(dim=1).indices[0]\n",
    "                # The action must have shape (1, 1) so that they can be concatenated\n",
    "                # when sampled from the replay memory\n",
    "                action = action.view(1, 1)\n",
    "        \n",
    "        self.last_state = grid\n",
    "        self.last_action = action\n",
    "        return int(action.item())\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b286b8-cf7f-48f8-8b23-ee1d7528f12c",
   "metadata": {},
   "source": [
    "## Game function for DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1de97-16f4-4347-9ffe-5bbf9b406105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid33_to_332(grid, player, player2value):\n",
    "    \"\"\"\n",
    "    Converts a grid in 3x3 shape whose values are -1, 0 and 1\n",
    "    to the format expected by the DQN player, as a 3x3x2 array.\n",
    "    --player: either 'X' or 'O', which player the DQN agent is.\n",
    "    --player2value: player (X or O) to index (-1 or 1) association,\n",
    "        obtained from the environment.\n",
    "    \"\"\"\n",
    "    grid_332 = np.zeros((3, 3, 2))\n",
    "    # Get the value in the original grid corresponding to the player\n",
    "    # played by the dqn agent:\n",
    "    player_ind = player2value[player]\n",
    "    \n",
    "    grid_332[grid == player_ind, 0] = 1\n",
    "    grid_332[grid == -player_ind, 1] = 1\n",
    "    return grid_332"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e81180-76b9-4494-a691-5018fe6f479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_games_dqn(dqn_player, benchmark_player, nb_games=20000,\n",
    "                   games_between_updates=500,\n",
    "                   turns_swap=\"switch\",\n",
    "                   seed=666):\n",
    "    \"\"\"\n",
    "    Plays a given number of games between two players, and returns the rewards.\n",
    "    --dqn_player: Instance of DQNPlayer to train;\n",
    "    --benchmark_player: Player object implementing act();\n",
    "    --nb_games: How many games should be played;\n",
    "    --games_between_updates: how many games are played between two updates of the agent's\n",
    "        target network.\n",
    "    --turns_swap: str, either \"switch\" to switch turns after every game, or \"random\".\n",
    "    --seed: random seed.\n",
    "    Returns two arrays: rewards, losses\n",
    "    \"\"\"\n",
    "    turns = np.array(['X','O'])\n",
    "    dqn_player.set_player(turns[0])\n",
    "    benchmark_player.set_player(turns[1])\n",
    "    rewards, losses = [], []\n",
    "    env = TictactoeEnv()\n",
    "    \n",
    "    for game in trange(nb_games):\n",
    "        # Sets up the environment for the game\n",
    "        env.reset()\n",
    "        grid, _, _ = env.observe()\n",
    "        # Convert the grid from the env's format to that expected by the agent\n",
    "        grid_tensor = grid33_to_332(grid, dqn_player.player, env.player2value)\n",
    "        grid_tensor = torch.tensor(grid_tensor, device=device).unsqueeze(0).float()\n",
    "        \n",
    "        if turns_swap == \"switch\":\n",
    "            turns = turns[[-1, 0]]\n",
    "        else:\n",
    "            turns = np.random.shuffle(turns)\n",
    "            \n",
    "        dqn_player.set_player(turns[0])\n",
    "        benchmark_player.set_player(turns[1])\n",
    "        \n",
    "        while True:\n",
    "            # Action step\n",
    "            # We now need to account for the case where the agent chooses\n",
    "            # an unavailable position.\n",
    "            if env.current_player == dqn_player.player:\n",
    "                try:\n",
    "                    move = dqn_player.act(grid_tensor)\n",
    "                    grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "                except ValueError:\n",
    "                    # Stop the game and set the reward for the agent to -1\n",
    "                    end = True\n",
    "                    reward = -1\n",
    "            else:\n",
    "                move = benchmark_player.act(grid)\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "                grid_tensor = grid33_to_332(grid, dqn_player.player, env.player2value)\n",
    "                grid_tensor = torch.tensor(grid_tensor, device=device).unsqueeze(0).float()\n",
    "                reward = env.reward(dqn_player.player)\n",
    "\n",
    "                # Learning step\n",
    "                # The DQN agent must have played at least once to start learning\n",
    "                if dqn_player.last_action is not None:\n",
    "                    losses.append(dqn_player.learn(torch.tensor([reward], device=device),\n",
    "                                                   grid_tensor))\n",
    "\n",
    "            if end:\n",
    "                env.reset()\n",
    "                rewards.append(reward)\n",
    "                break\n",
    "            \n",
    "        # Update the agent's target network if required\n",
    "        if game % games_between_updates == 0:\n",
    "            dqn_player.update()\n",
    "    \n",
    "    return np.array(rewards), np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230333e9-692a-45bc-8112-4a0bc2e413d1",
   "metadata": {},
   "source": [
    "### 3.2 Learning from experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d785c-faf2-48f2-8e28-a67f6a77c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_player = DQNPlayer(epsilon=0.05)\n",
    "rewards, losses = play_games_dqn(dqn_player, semi_random_player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd55d4f-975b-4653-9f4e-fff8929cb5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(windowed_avg(rewards))\n",
    "plt.ylim(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52247300-b635-4262-a8df-b703aad442e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
