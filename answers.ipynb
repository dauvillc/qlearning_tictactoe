{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58fa93b-5307-40b0-9b87-1896707c25b9",
   "metadata": {},
   "source": [
    "# Mini-project 1: Tic-Tac-Toe\n",
    "Cl√©ment DAUVILLIERS - Florian VINCENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14bd7b-e0ed-415d-a44f-a013a1b5ca3a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250cc033-1d30-4b7f-9f5b-1314532de708",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b31125c7-3f29-43bb-a467-9caaa336a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange\n",
    "from math import log\n",
    "from copy import deepcopy\n",
    "from queue import deque\n",
    "from random import sample\n",
    "from tic_env import TictactoeEnv, OptimalPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b2b5e6-3607-4252-b997-67dcb8d1646a",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e20fe206-dcef-43bb-865c-f7d9dfda47c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_avg(arr, window_len=250):\n",
    "    \"\"\"\n",
    "    Computes the average over successive windows of an array.\n",
    "    arr must be a 1D array whose length is a multiple of the\n",
    "    window length.\n",
    "    \"\"\"\n",
    "    return arr.reshape((window_len, -1)).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c32c475-22d4-4e03-9dac-d32b5adef9eb",
   "metadata": {},
   "source": [
    "## Player class\n",
    "The following class will be used as base for the QLearning and DQN player classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b11f77-858b-4ce3-a796-c5612e69154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    \"\"\"\n",
    "    Base class for both types of players (QLearning, DQN).\n",
    "    \"\"\"\n",
    "    def __init__(self, player='X', epsilon=0.05, seed=666):\n",
    "        self.player = player\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # RNG for the epsilon-gredy policy\n",
    "        self.rng_ = np.random.default_rng(seed=seed)\n",
    "    \n",
    "    def act(self, grid):\n",
    "        \"\"\"\n",
    "        Selects an action to perform based on the current\n",
    "        grid state and the player's policy.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Call from abstract class\")\n",
    "        \n",
    "    def set_player(self, player):\n",
    "        self.player = player\n",
    "    \n",
    "    @staticmethod\n",
    "    def empty(grid):\n",
    "        '''return all empty positions'''\n",
    "        avail = []\n",
    "        for i in range(9):\n",
    "            pos = (int(i/3), i % 3)\n",
    "            if grid[pos] == 0:\n",
    "                avail.append(pos)\n",
    "        return avail\n",
    "        \n",
    "    def randomMove(self, grid):\n",
    "        \"\"\" Chose a random move from the available options. \"\"\"\n",
    "        avail = self.empty(grid)\n",
    "\n",
    "        return avail[self.rng_.integers(0, len(avail))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1499d2e6-ceb4-4b41-93b3-f7459700b195",
   "metadata": {},
   "source": [
    "# 2. Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb32df3e-82d9-4c5d-8fbe-2b7d7b19ebe0",
   "metadata": {},
   "source": [
    "### QLPlayer class\n",
    "The following class implements the QLearning player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22d931d5-1c3d-46e8-b6b1-d15ffcbfb2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLPlayer(Player):\n",
    "    \"\"\"\n",
    "    Implements a player that learns using the QLearning algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, player='X', lr=0.05, discount=0.99, epsilon=0.05, seed=666):\n",
    "        super().__init__(player, epsilon, seed)\n",
    "        self.lr = lr\n",
    "        self.discount = discount\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Q-values grid\n",
    "        # 3^9 = 19683 states and 9 actions\n",
    "        self.qvalues = np.zeros((19683, 9))\n",
    "        \n",
    "        # Memory\n",
    "        self.last_action = None\n",
    "        self.last_state = None\n",
    "        \n",
    "    def act(self, grid):\n",
    "        # Inverts the grid if required\n",
    "        grid = self.invert_grid(grid)\n",
    "        \n",
    "        # Epsilon-greedy choice\n",
    "        if self.rng_.random() < self.epsilon:\n",
    "            return self.randomMove(grid)\n",
    "        # Retrieves the list of possible actions and converts them\n",
    "        # from cell positions to integer indexes\n",
    "        avail_actions = QLPlayer.positions_to_ints(Player.empty(grid))\n",
    "        # Ranks ALL actions according to their Qvalues in the current\n",
    "        # state\n",
    "        state = QLPlayer.state_to_int(grid)\n",
    "        actions_ranks = np.argsort(self.qvalues[state])[::-1]\n",
    "        # Browses all actions in order of their qvalue rank, until\n",
    "        # finding one that is available\n",
    "        for action in actions_ranks:\n",
    "            if action in avail_actions:\n",
    "                # Memorizes the action and the current state for the learning\n",
    "                # phase\n",
    "                self.last_action, self.last_state = action, state\n",
    "                return int(action)\n",
    "    \n",
    "    def learn(self, reward, new_grid, end):\n",
    "        \"\"\"\n",
    "        Updates the Qvalues based on the last (S, A) pair and\n",
    "        the received reward and the new state.\n",
    "        \"\"\"\n",
    "        # If the new_grid is a final state, we can't compute its expected optimal\n",
    "        # qvalue. We instead set it to zero.\n",
    "        if end:\n",
    "            new_state_qval = 0\n",
    "        else:\n",
    "            # Inverts the grid if required (so that ones corresponding\n",
    "            # to THIS player's chesses).\n",
    "            new_grid = self.invert_grid(new_grid)\n",
    "\n",
    "            # Computes the optimal Qvalue in the new state max Q(s', a)\n",
    "            new_state = QLPlayer.state_to_int(new_grid)\n",
    "            new_state_qval = np.max(self.qvalues[new_state])\n",
    "        \n",
    "        # QValue that needs to be updated Q(s, a)\n",
    "        current_qval = self.qvalues[self.last_state, self.last_action]\n",
    "        \n",
    "        self.qvalues[self.last_state, self.last_action] += self.lr * (reward + self.discount * new_state_qval - current_qval)\n",
    "    \n",
    "    def invert_grid(self, grid):\n",
    "        \"\"\"\n",
    "        Returns a version of the grid in which the ones correspond to this player's\n",
    "        chesses.\n",
    "        \"\"\"\n",
    "        # If we play with the 'O', then the -1 in the grid are actually our pieces\n",
    "        return grid if self.player == 'X' else -grid\n",
    "    \n",
    "    @staticmethod\n",
    "    def position_to_int(position):\n",
    "        \"\"\"\n",
    "        (row col) -> row*3 + col\n",
    "        \"\"\"\n",
    "        return position[0] * 3 + position[1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def positions_to_ints(positions):\n",
    "        \"\"\"\n",
    "        Given a list of cells [(a, b), (c, d), ..],\n",
    "        returns the list of the corresponding indexes.\n",
    "        \"\"\"\n",
    "        return [QLPlayer.position_to_int(cell) for cell in positions]\n",
    "\n",
    "    @staticmethod\n",
    "    def state_to_int(grid):\n",
    "        \"\"\"\n",
    "        Converts a grid state to the index of its\n",
    "        row in the lookup table.\n",
    "        \"\"\"\n",
    "        # Converts the grid values from -1, 0, 1 to 0, 1, 2 (a base 3 number)\n",
    "        # Then converts the base 3 number to base 10\n",
    "        return int((np.ravel(grid) + 1) @ np.array([3 ** i for i in range(9)]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def int_to_state(state_int):\n",
    "        \"\"\"\n",
    "        Converts the index of row in the qvalues table to\n",
    "        its corresponding state.\n",
    "        \"\"\"\n",
    "        # Converts from base 10 to base 3\n",
    "        return np.array([\n",
    "            (state_int % (3 ** (i + 1))) // (3 ** i)\n",
    "            for i in range(9)\n",
    "        ]).reshape((3, 3)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b2b39-d3e1-4de5-abf7-6fd73cd57acf",
   "metadata": {},
   "source": [
    "## 2.1 Learning from experts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96654ae4-a652-4435-945b-5a82a533c250",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ea95936-f056-43f1-93af-1072a940bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_games(learning_player, benchmark_player, nb_games=20000, turns_swap=\"switch\",\n",
    "               seed=666, learn=True):\n",
    "    \"\"\"\n",
    "    Plays a given number of games between two players, and returns the rewards.\n",
    "    --learning_player: Player object implementing act(), learn(), update();\n",
    "    --benchmark_player: Player object implementing act();\n",
    "    --nb_games: How many games should be played;\n",
    "    --turns_swap: str, either \"switch\" to switch turns after every game, or \"random\".\n",
    "    --seed: random seed.\n",
    "    \"\"\"\n",
    "    turns = np.array(['X','O'])\n",
    "    learning_player.set_player(turns[0])\n",
    "    benchmark_player.set_player(turns[1])\n",
    "    rewards = []\n",
    "    env = TictactoeEnv()\n",
    "    \n",
    "    for game in trange(nb_games):\n",
    "        # Sets up the environment for the game\n",
    "        env.reset()\n",
    "        grid, _, _ = env.observe()\n",
    "        if turns_swap == \"switch\":\n",
    "            turns = turns[[-1, 0]]\n",
    "        else:\n",
    "            turns = np.random.shuffle(turns)\n",
    "        learning_player.set_player(turns[0])\n",
    "        benchmark_player.set_player(turns[1])\n",
    "        \n",
    "        while True:\n",
    "            # Action step\n",
    "            if env.current_player == learning_player.player:\n",
    "                move = learning_player.act(grid)\n",
    "            else:\n",
    "                move = benchmark_player.act(grid)\n",
    "\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "            reward = env.reward(learning_player.player)\n",
    "\n",
    "            # Learning step\n",
    "            # The agent learns only after the other has played, as from the\n",
    "            # point of view of the agent, the next state is not the one right after\n",
    "            # its move, but the next state in which the agent will need to make a decision.\n",
    "            if (env.current_player == benchmark_player.player or end) and learn:\n",
    "                learning_player.learn(reward, grid, end)\n",
    "\n",
    "            if end:\n",
    "                env.reset()\n",
    "                rewards.append(reward)\n",
    "                break\n",
    "    \n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e865c195-e3d9-4a6a-9603-c233a9471915",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlplayer = QLPlayer(epsilon=0.1)\n",
    "semi_random_player = OptimalPlayer(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cc53ea42-3aad-44b6-abd6-55e7aab286ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb03be8d17e49c5a9b5be5b60b456cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards_21 = play_games(qlplayer, semi_random_player, nb_games=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0a3a7434-9f97-4867-8cfc-ca93c850d876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(windowed_avg(rewards_21))\n",
    "plt.ylim(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5242a9-7f44-4147-80de-4bd8ca20a9a0",
   "metadata": {},
   "source": [
    "# 3. Deep Q-Learning\n",
    "All of the following implementations will be based on the PyTorch RL Tutorial:  \n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.htmlhttps://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ce705-50db-4f79-9ca4-c1b582865869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79746898-a689-4fd0-abee-6c7897c628cf",
   "metadata": {},
   "source": [
    "We'll try to work on a GPU if one is available to PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27fc3e-b8aa-4a4c-b391-7ebecf264f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315c8eb-2938-48a3-8959-507049afcd7c",
   "metadata": {},
   "source": [
    "## Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f38b6-dda0-4e27-ae44-498ac117aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    state (3,3,2)->flatten->(18,) nn input\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.input = nn.Linear(18, 128)\n",
    "        self.hidden1 = nn.Linear(128, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.output = nn.Linear(128, 9)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        # Flattens x to make sure it can be passed to the linear layers\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        # No activation is a linear activation\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2681db-9713-440a-bbea-73a865d205d9",
   "metadata": {},
   "source": [
    "## Replay memory\n",
    "This class is directly taken from the PyTorch DQN tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf7201-8a61-456b-a638-eff02c09e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5455aa26-ee1a-4d0b-b329-284c6b75278f",
   "metadata": {},
   "source": [
    "## DQNPlayer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f19284-361f-400f-9f62-66dedd5e9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNPlayer(Player):\n",
    "    \"\"\"\n",
    "    Implements a type of Player that uses Deep Q-Learning\n",
    "    to learn the tictactoe strategy.\n",
    "    \"\"\"\n",
    "    def __init__(self, player='X', lr=5e-4, discount=0.99, epsilon=0.05, batch_size=64,\n",
    "                  seed=666):\n",
    "        super().__init__(player, epsilon, seed)\n",
    "        self.lr = lr\n",
    "        self.discount = discount\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.last_action, self.last_state = None, None\n",
    "        \n",
    "        # Neural networks\n",
    "        self.policy_net = DQN().to(device)\n",
    "        self.target_net = DQN().to(device)\n",
    "        \n",
    "        self.memory = ReplayMemory(10000)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Huber loss\n",
    "        self.criterion = nn.HuberLoss()\n",
    "        # Adam optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
    "    \n",
    "    def learn(self, reward, grid):\n",
    "        \"\"\"\n",
    "        Stores the last (S, A, NS, R) tuple into the replay memory,\n",
    "        and trains the policy network using a sample from the replay memory.\n",
    "        --reward: float, end game reward\n",
    "        --grid: (3, 3, 2) array representing the current state.\n",
    "        Returns the value of the Huber loss.\n",
    "        \"\"\"\n",
    "        # Push the last experience into the replay memory\n",
    "        self.memory.push(self.last_state, self.last_action, grid, reward)\n",
    "        \n",
    "        # We don't start learning before the replay memory is large enough to\n",
    "        # return at least a full batch\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        ## Policy network training ============================================\n",
    "        # Taken from the Pytorch RL tutorial\n",
    "        # First sample a batch of Transition objects\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # Then creates a single Transition obj whose elements are arrays\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        next_state_batch = torch.cat(batch.next_state)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        # We need to know where in the batch the next state is final\n",
    "        # as we can't compute max Q(ns, .) for those.\n",
    "        # The final states are characterized by having the sum of the grid equal to 9\n",
    "        # (1 for each cell).\n",
    "        non_final_mask = next_state_batch.sum((1, 2, 3)) < 9\n",
    "        non_final_next_states = next_state_batch[non_final_mask]\n",
    "        \n",
    "        # Computes the state-action values for all actions for all states in the batch\n",
    "        state_action_values = self.policy_net(state_batch)\n",
    "        # For each state, selects only Q(s, a) for the a which was actually chosen\n",
    "        state_action_values = state_action_values.gather(1, action_batch)\n",
    "        \n",
    "        # We now need to compute max_a Q(s', a)\n",
    "        next_state_qvalues = torch.zeros(self.batch_size, device=device)\n",
    "        # We'll set it to zero for final states\n",
    "        # Make sure to use the target network (not the policy) for training stability.\n",
    "        # Note that tensor.max(dim=...) returns a namedtuple (values, indices)\n",
    "        next_state_qvalues[non_final_mask] = self.target_net(non_final_next_states).max(dim=1).values\n",
    "        # Detach the next state values from the gradient graph as it will be used\n",
    "        # as the target in the computation of the loss (We consider it as the \"true qvalue\"\n",
    "        # and hope to converge towards the Bellman equation).\n",
    "        next_state_qvalues = next_state_qvalues.detach()\n",
    "        \n",
    "        # Final objective term\n",
    "        target = reward_batch + self.discount * next_state_qvalues\n",
    "        \n",
    "        # Loss minimization using the optimizer (usual PyTorch training phase)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(state_action_values, target.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # Returns the loss as a float value\n",
    "        return loss.item()\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Udpates the target network by setting its weights\n",
    "        to those of the policy network.\n",
    "        \"\"\"\n",
    "        # We need to make a copy of the policy net's state_dict,\n",
    "        # otherwise we'll keep updating the target net at each iteration\n",
    "        state_dict = self.policy_net.state_dict()\n",
    "        # Calling deepcopy() seems to be the \"best\" way:\n",
    "        # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "        self.target_net.load_state_dict(deepcopy(state_dict))\n",
    "    \n",
    "    def act(self, grid):\n",
    "        \"\"\"\n",
    "        Chooses the action to perform by taking that which returns the\n",
    "        best qvalue, as estimated by the policy network.\n",
    "        Returns the action taken as an integer from 0 to 8.\n",
    "        \"\"\"\n",
    "        # Check whether the epsilon-greedy choice activates\n",
    "        if self.rng_.random() < self.epsilon:\n",
    "            action = torch.tensor([[self.rng_.integers(0, 9)]], device=device)\n",
    "        else:\n",
    "            # We don't want those computations to impact the gradient graph\n",
    "            # somehow\n",
    "            with torch.no_grad():\n",
    "                qvalues = self.policy_net(grid)\n",
    "                # Select the action that has the highest qvalue\n",
    "                # Note that tensor.max(dim=...) returns a namedtuple (values, indices)\n",
    "                action = qvalues.max(dim=1).indices[0]\n",
    "                # The action must have shape (1, 1) so that they can be concatenated\n",
    "                # when sampled from the replay memory\n",
    "                action = action.view(1, 1)\n",
    "        \n",
    "        self.last_state = grid\n",
    "        self.last_action = action\n",
    "        return int(action.item())\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b286b8-cf7f-48f8-8b23-ee1d7528f12c",
   "metadata": {},
   "source": [
    "## Game function for DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1de97-16f4-4347-9ffe-5bbf9b406105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid33_to_332(grid, player, player2value):\n",
    "    \"\"\"\n",
    "    Converts a grid in 3x3 shape whose values are -1, 0 and 1\n",
    "    to the format expected by the DQN player, as a 3x3x2 array.\n",
    "    --player: either 'X' or 'O', which player the DQN agent is.\n",
    "    --player2value: player (X or O) to index (-1 or 1) association,\n",
    "        obtained from the environment.\n",
    "    \"\"\"\n",
    "    grid_332 = np.zeros((3, 3, 2))\n",
    "    # Get the value in the original grid corresponding to the player\n",
    "    # played by the dqn agent:\n",
    "    player_ind = player2value[player]\n",
    "    \n",
    "    grid_332[grid == player_ind, 0] = 1\n",
    "    grid_332[grid == -player_ind, 1] = 1\n",
    "    return grid_332"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e81180-76b9-4494-a691-5018fe6f479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_games_dqn(dqn_player, benchmark_player, nb_games=20000,\n",
    "                   games_between_updates=500,\n",
    "                   turns_swap=\"switch\",\n",
    "                   seed=666):\n",
    "    \"\"\"\n",
    "    Plays a given number of games between two players, and returns the rewards.\n",
    "    --dqn_player: Instance of DQNPlayer to train;\n",
    "    --benchmark_player: Player object implementing act();\n",
    "    --nb_games: How many games should be played;\n",
    "    --games_between_updates: how many games are played between two updates of the agent's\n",
    "        target network.\n",
    "    --turns_swap: str, either \"switch\" to switch turns after every game, or \"random\".\n",
    "    --seed: random seed.\n",
    "    Returns two arrays: rewards, losses\n",
    "    \"\"\"\n",
    "    turns = np.array(['X','O'])\n",
    "    dqn_player.set_player(turns[0])\n",
    "    benchmark_player.set_player(turns[1])\n",
    "    rewards, losses = [], []\n",
    "    env = TictactoeEnv()\n",
    "    \n",
    "    for game in trange(nb_games):\n",
    "        # Sets up the environment for the game\n",
    "        env.reset()\n",
    "        grid, _, _ = env.observe()\n",
    "        # Convert the grid from the env's format to that expected by the agent\n",
    "        grid_tensor = grid33_to_332(grid, dqn_player.player, env.player2value)\n",
    "        grid_tensor = torch.tensor(grid_tensor, device=device).unsqueeze(0).float()\n",
    "        \n",
    "        if turns_swap == \"switch\":\n",
    "            turns = turns[[-1, 0]]\n",
    "        else:\n",
    "            turns = np.random.shuffle(turns)\n",
    "            \n",
    "        dqn_player.set_player(turns[0])\n",
    "        benchmark_player.set_player(turns[1])\n",
    "        \n",
    "        while True:\n",
    "            # Action step\n",
    "            # We now need to account for the case where the agent chooses\n",
    "            # an unavailable position.\n",
    "            if env.current_player == dqn_player.player:\n",
    "                try:\n",
    "                    move = dqn_player.act(grid_tensor)\n",
    "                    grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "                except ValueError:\n",
    "                    # Stop the game and set the reward for the agent to -1\n",
    "                    end = True\n",
    "                    reward = -1\n",
    "            else:\n",
    "                move = benchmark_player.act(grid)\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "                grid_tensor = grid33_to_332(grid, dqn_player.player, env.player2value)\n",
    "                grid_tensor = torch.tensor(grid_tensor, device=device).unsqueeze(0).float()\n",
    "                reward = env.reward(dqn_player.player)\n",
    "\n",
    "                # Learning step\n",
    "                # The DQN agent must have played at least once to start learning\n",
    "                if dqn_player.last_action is not None:\n",
    "                    losses.append(dqn_player.learn(torch.tensor([reward], device=device),\n",
    "                                                   grid_tensor))\n",
    "\n",
    "            if end:\n",
    "                env.reset()\n",
    "                rewards.append(reward)\n",
    "                break\n",
    "            \n",
    "        # Update the agent's target network if required\n",
    "        if game % games_between_updates == 0:\n",
    "            dqn_player.update()\n",
    "    \n",
    "    return np.array(rewards), np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230333e9-692a-45bc-8112-4a0bc2e413d1",
   "metadata": {},
   "source": [
    "### 3.2 Learning from experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d785c-faf2-48f2-8e28-a67f6a77c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_player = DQNPlayer(epsilon=0.05)\n",
    "rewards, losses = play_games_dqn(dqn_player, semi_random_player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd55d4f-975b-4653-9f4e-fff8929cb5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(windowed_avg(rewards))\n",
    "plt.ylim(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52247300-b635-4262-a8df-b703aad442e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
