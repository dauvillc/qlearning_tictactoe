{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58fa93b-5307-40b0-9b87-1896707c25b9",
   "metadata": {},
   "source": [
    "# Mini-project 1: Tic-Tac-Toe\n",
    "ClÃ©ment DAUVILLIERS - Florian VINCENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14bd7b-e0ed-415d-a44f-a013a1b5ca3a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31125c7-3f29-43bb-a467-9caaa336a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.typing as npt\n",
    "from tqdm.notebook import trange\n",
    "from copy import deepcopy\n",
    "from queue import deque\n",
    "from random import sample\n",
    "from typing import List, Tuple, NewType, Union, Callable\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "from player import Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ce705-50db-4f79-9ca4-c1b582865869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250cc033-1d30-4b7f-9f5b-1314532de708",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86870d16-c73e-44e9-97fe-ad9346f7c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid = NewType('Grid', npt.NDArray[np.float64])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b2b5e6-3607-4252-b997-67dcb8d1646a",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20fe206-dcef-43bb-865c-f7d9dfda47c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_avg(arr: npt.ArrayLike, window_len: int=250) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Computes the average over successive windows of an array.\n",
    "    arr must be a 1D array whose length is a multiple of the\n",
    "    window length.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for win_start in range(0, arr.shape[0], window_len):\n",
    "        result.append(np.mean(arr[win_start:win_start + window_len]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1499d2e6-ceb4-4b41-93b3-f7459700b195",
   "metadata": {},
   "source": [
    "# 2. Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb32df3e-82d9-4c5d-8fbe-2b7d7b19ebe0",
   "metadata": {},
   "source": [
    "### QLPlayer class\n",
    "The following class implements the QLearning player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d931d5-1c3d-46e8-b6b1-d15ffcbfb2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLPlayer(Player):\n",
    "    \"\"\"\n",
    "    Implements a player that learns using the QLearning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, player='X',\n",
    "                 lr: float = 0.05,\n",
    "                 discount: float = 0.99,\n",
    "                 epsilon: Callable[[int], float] = lambda _: 0.05,\n",
    "                 seed: int = 666):\n",
    "        super().__init__(player, epsilon, seed)\n",
    "        self.lr = lr\n",
    "        self.discount = discount\n",
    "\n",
    "        # Q-values grid\n",
    "        # 3^9 = 19683 states and 9 actions\n",
    "        self.qvalues: Grid = np.zeros((19683, 9))\n",
    "\n",
    "        # Memory\n",
    "        # Starts with None before any action and state are ever seen\n",
    "        self.last_action: Union[None, int] = None\n",
    "        self.last_state: Union[None, Grid] = None\n",
    "\n",
    "    def act(self,\n",
    "            grid: Grid,\n",
    "            iteration: int = 0):\n",
    "        # Remember the state for the next learning step\n",
    "        state = QLPlayer.state_to_int(grid)\n",
    "        self.last_state = state\n",
    "\n",
    "        # Epsilon-greedy choice\n",
    "        if self.rng_.random() < self.epsilon(iteration):\n",
    "            chosen_action = self.randomMove(grid)\n",
    "        else:\n",
    "            # Retrieves the list of possible actions and converts them\n",
    "            # from cell positions to integer indexes\n",
    "            avail_actions = QLPlayer.positions_to_ints(Player.empty(grid))\n",
    "            # Ranks ALL actions according to their Qvalues in the current\n",
    "            # state\n",
    "            actions_ranks = np.argsort(self.qvalues[state])[::-1]\n",
    "            # Browses all actions in order of their qvalue rank, until\n",
    "            # finding one that is available\n",
    "            for action in actions_ranks:\n",
    "                if action in avail_actions:\n",
    "                    # Memorizes the action and the current state for the learning\n",
    "                    # phase\n",
    "                    chosen_action = int(action)\n",
    "                    break\n",
    "\n",
    "        # Remember the action for the learning step\n",
    "        self.last_action = chosen_action\n",
    "        return chosen_action\n",
    "\n",
    "    def learn(self, reward, new_grid, end):\n",
    "        \"\"\"\n",
    "        Updates the Qvalues based on the last (S, A) pair and\n",
    "        the received reward and the new state.\n",
    "        \"\"\"\n",
    "        # If the new_grid is a final state, we can't compute its expected optimal\n",
    "        # qvalue. We instead set it to zero.\n",
    "        if end:\n",
    "            new_state_qval: int = 0\n",
    "        else:\n",
    "            # Computes the optimal Qvalue in the new state max Q(s', a)\n",
    "            new_state: int = QLPlayer.state_to_int(new_grid)\n",
    "            new_state_qval: np.float64 = np.max(self.qvalues[new_state])\n",
    "\n",
    "        # QValue that needs to be updated Q(s, a)\n",
    "        current_qval: np.float64 = self.qvalues[self.last_state, self.last_action]\n",
    "\n",
    "        self.qvalues[self.last_state, self.last_action] += self.lr * (reward + self.discount * new_state_qval - current_qval)\n",
    "\n",
    "    @staticmethod\n",
    "    def position_to_int(position: Tuple[int, int]) -> int:\n",
    "        \"\"\"\n",
    "        (row col) -> row*3 + col\n",
    "        \"\"\"\n",
    "        return position[0] * 3 + position[1]\n",
    "\n",
    "    @staticmethod\n",
    "    def positions_to_ints(positions: List[Tuple[int, int]]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Given a list of cells [(a, b), (c, d), ..],\n",
    "        returns the list of the corresponding indexes.\n",
    "        \"\"\"\n",
    "        return [QLPlayer.position_to_int(cell) for cell in positions]\n",
    "\n",
    "    @staticmethod\n",
    "    def state_to_int(grid: Grid) -> int:\n",
    "        \"\"\"\n",
    "        Converts a grid state to the index of its\n",
    "        row in the lookup table.\n",
    "        \"\"\"\n",
    "        # Converts the grid values from -1, 0, 1 to 0, 1, 2 (a base 3 number)\n",
    "        # Then converts the base 3 number to base 10\n",
    "        return int((np.ravel(grid) + 1) @ np.array([3 ** i for i in range(9)]))\n",
    "\n",
    "    @staticmethod\n",
    "    def int_to_state(state_int: int) -> Grid:\n",
    "        \"\"\"\n",
    "        Converts the index of row in the qvalues table to\n",
    "        its corresponding state.\n",
    "        \"\"\"\n",
    "        # Converts from base 10 to base 3\n",
    "        return np.array([\n",
    "            (state_int % (3 ** (i + 1))) // (3 ** i)\n",
    "            for i in range(9)\n",
    "        ]).reshape((3, 3)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b2b39-d3e1-4de5-abf7-6fd73cd57acf",
   "metadata": {},
   "source": [
    "## 2.1 Learning from experts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96654ae4-a652-4435-945b-5a82a533c250",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea95936-f056-43f1-93af-1072a940bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_games(learning_player: Union[Player, OptimalPlayer],\n",
    "               benchmark_player: Union[Player, OptimalPlayer],\n",
    "               nb_games: int = 20000,\n",
    "               turns_swap: str = \"switch\",\n",
    "               seed: int = 666,\n",
    "               learn: bool = True,\n",
    "               period_Ms: int = 0,\n",
    "               progress=True) -> Tuple[npt.NDArray[np.int_],\n",
    "                                            npt.NDArray[np.float32]]:\n",
    "    \"\"\"\n",
    "    Plays a given number of games between two players, and returns the rewards.\n",
    "    --learning_player: Player object implementing act(), learn(), update();\n",
    "    --benchmark_player: Player object implementing act();\n",
    "    --nb_games: How many games should be played;\n",
    "    --turns_swap: str, either \"switch\" to switch turns after every game,\n",
    "                    or \"random\".\n",
    "    --seed: random seed.\n",
    "    \"\"\"\n",
    "    turns = np.array(['X', 'O'])\n",
    "    learning_player.set_player(turns[0])\n",
    "    benchmark_player.set_player(turns[1])\n",
    "    rewards: List[int] = []\n",
    "    env = TictactoeEnv()\n",
    "    game_swap = nb_games // 2 if turns_swap=='half' else None\n",
    "    if period_Ms:\n",
    "        Ms = []\n",
    "    else:\n",
    "        Ms = None\n",
    "    if not progress:\n",
    "        iterator = range(nb_games)\n",
    "    else:\n",
    "        iterator = trange(nb_games)\n",
    "\n",
    "    for game in iterator:\n",
    "        # Sets up the environment for the game\n",
    "        env.reset()\n",
    "        grid: Grid = env.observe()[0]\n",
    "        if turns_swap == \"switch\":\n",
    "            learning_player.set_player(j=game)\n",
    "            benchmark_player.set_player(j=game + 1)\n",
    "        elif turns_swap == \"half\":\n",
    "            learning_player.set_player(j=int(game < game_swap))\n",
    "            learning_player.set_player(j=int(game >= game_swap))\n",
    "        else:\n",
    "            turns = np.random.shuffle(turns)\n",
    "            learning_player.set_player(turns[0])\n",
    "            benchmark_player.set_player(turns[1])\n",
    "\n",
    "        while True:\n",
    "            # Action step\n",
    "            if env.current_player == learning_player.player:\n",
    "                move: int = learning_player.act(grid, game)\n",
    "            else:\n",
    "                move: int = benchmark_player.act(grid)\n",
    "\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "            reward: int = env.reward(learning_player.player)\n",
    "\n",
    "            # Learning step\n",
    "            # The agent learns only after the other has played, as from the\n",
    "            # point of view of the agent, the next state is not the one right after\n",
    "            # its move, but the next state in which the agent will need to make a decision.\n",
    "            # if current player == learning player means the benchmark player just played !\n",
    "            if (env.current_player == learning_player.player or end) and learn:\n",
    "                learning_player.learn(reward, grid, end)\n",
    "\n",
    "            if end:\n",
    "                rewards.append(reward)\n",
    "                break\n",
    "\n",
    "        if period_Ms * game and game % period_Ms == 0:\n",
    "            Ms.append(get_Ms(learning_player,\n",
    "                             n_test=500,\n",
    "                             seed=seed))\n",
    "\n",
    "    return np.array(rewards), np.array(Ms)\n",
    "\n",
    "\n",
    "def get_Ms(learning_player: Union[Player, OptimalPlayer],\n",
    "           n_test: int = 500,\n",
    "           seed: int = 666) -> List[float]:\n",
    "    rewards_opt: npt.NDArray[np.int_] = \\\n",
    "        play_games(learning_player=learning_player,\n",
    "                   benchmark_player=OptimalPlayer(0.),\n",
    "                   nb_games=n_test,\n",
    "                   turns_swap='half',\n",
    "                   seed=seed,\n",
    "                   learn=False,\n",
    "                   period_Ms=0,\n",
    "                   progress=False)[0]\n",
    "    rewards_rand: npt.NDArray[np.int_] = \\\n",
    "        play_games(learning_player=learning_player,\n",
    "                   benchmark_player=OptimalPlayer(1.),\n",
    "                   nb_games=n_test,\n",
    "                   turns_swap='half',\n",
    "                   seed=seed,\n",
    "                   learn=False,\n",
    "                   period_Ms=0,\n",
    "                   progress=False)[0]\n",
    "    return [rewards_opt.mean(), rewards_rand.mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30906537-06fc-428b-b393-2306165b7736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_epsilon(value):\n",
    "    \"\"\"\n",
    "    Returns a function for epsilon whose value\n",
    "    is independent of the iteration.\n",
    "    \"\"\"\n",
    "    return lambda iteration: value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53ea42-3aad-44b6-abd6-55e7aab286ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_games = 20000\n",
    "# Defines a QLearning Agent with constant epsilon of 0.05\n",
    "qlplayer = QLPlayer(epsilon=constant_epsilon(0.05), lr=0.05, discount=0.99)\n",
    "semi_random_player = OptimalPlayer(0.5)\n",
    "rewards_21 = play_games(qlplayer, semi_random_player, nb_games=nb_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a7434-9f97-4867-8cfc-ca93c850d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 250\n",
    "plt.plot(windowed_avg(rewards_21, window_size))\n",
    "plt.ylabel('Average final reward over 250 games')\n",
    "plt.ylim(-1, 1)\n",
    "plt.xlabel('Games played')\n",
    "xticks = np.arange(nb_games // window_size + 1, step=10)\n",
    "plt.xticks(xticks, xticks * window_size)\n",
    "plt.suptitle(\"QLearning vs Optimal(0.5)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed8a3c9-f76e-4056-87eb-430a3ce56ba3",
   "metadata": {},
   "source": [
    "### 2.1.1 Decreasing exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b103aa8-a8ad-4619-9799-2dff58a3187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_epsilon(n_star: int,\n",
    "                       e_min: float = .1,\n",
    "                       e_max: float = .8) -> Callable[[int], float]:\n",
    "    return lambda n: max(e_min, e_max * (1. - float(n) / float(n_star)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5437bec-51a8-41ba-8a50-eb57639811e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_games = 20000\n",
    "list_Ms_plots = []\n",
    "for n_star, c in zip((1, 10, 100, 1000, 10000, 20000),\n",
    "                     ('r', 'b', 'g', 'c', 'k', 'y')):\n",
    "    print(\"Launch games for n*=\", n_star)\n",
    "    qlplayer = QLPlayer(epsilon=decreasing_epsilon(n_star))\n",
    "    semi_random_player = OptimalPlayer(0.5)\n",
    "    rewards_21, Ms = play_games(qlplayer,\n",
    "                                semi_random_player,\n",
    "                                nb_games=nb_games,\n",
    "                                period_Ms=250)\n",
    "    print(\"End of games for n*=\", n_star)\n",
    "    plt.plot(windowed_avg(rewards_21, window_size),\n",
    "             label=f'$n^*=${n_star}', c=c)\n",
    "    list_Ms_plots.append(Ms)\n",
    "plt.ylabel('Average final reward over 250 games for variable $\\\\epsilon(n)$')\n",
    "plt.ylim(-1, 1)\n",
    "plt.xlabel('Games played')\n",
    "xticks = np.arange(nb_games // window_size + 1, step=10)\n",
    "plt.xticks(xticks, xticks * window_size)\n",
    "plt.suptitle(\"QLearning vs Optimal(0.5)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5242a9-7f44-4147-80de-4bd8ca20a9a0",
   "metadata": {},
   "source": [
    "# 3. Deep Q-Learning\n",
    "All of the following implementations will be based on the PyTorch RL Tutorial:  \n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.htmlhttps://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7abb642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqnplayer import DQNPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79746898-a689-4fd0-abee-6c7897c628cf",
   "metadata": {},
   "source": [
    "We'll try to work on a GPU if one is available to PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27fc3e-b8aa-4a4c-b391-7ebecf264f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b286b8-cf7f-48f8-8b23-ee1d7528f12c",
   "metadata": {},
   "source": [
    "## Game function for DQN\n",
    "We'll now write the function that wraps the whole training loop.  \n",
    "First, we need two functions to manipulate states and make them coherent with the format expected by the DQN agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1de97-16f4-4347-9ffe-5bbf9b406105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid33_to_332(grid, player, player2value):\n",
    "    \"\"\"\n",
    "    Converts a grid in 3x3 shape whose values are -1, 0 and 1\n",
    "    to the format expected by the DQN player, as a 3x3x2 array.\n",
    "    --player: either 'X' or 'O', which player the DQN agent is.\n",
    "    --player2value: player (X or O) to index (-1 or 1) association,\n",
    "        obtained from the environment.\n",
    "    \"\"\"\n",
    "    grid_332 = np.zeros((3, 3, 2))\n",
    "    # Get the value in the original grid corresponding to the player\n",
    "    # played by the dqn agent:\n",
    "    player_ind = player2value[player]\n",
    "\n",
    "    grid_332[grid == player_ind, 0] = 1\n",
    "    grid_332[grid == -player_ind, 1] = 1\n",
    "    return grid_332\n",
    "\n",
    "def grid_to_tensor(grid, env):\n",
    "    \"\"\"\n",
    "    Converts the numpy array grid returned by the environment to a torch\n",
    "    tensor in the format expected by the DQN agent.\n",
    "    \"\"\"\n",
    "    grid_tensor = grid33_to_332(grid, dqn_player.player, env.player2value)\n",
    "    grid_tensor = torch.tensor(grid_tensor, device=device).unsqueeze(0).float()\n",
    "    return grid_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c1e36-b6cc-482a-9adb-2c67aa0ca7ac",
   "metadata": {},
   "source": [
    "We can now write the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e81180-76b9-4494-a691-5018fe6f479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_games_dqn(dqn_player, benchmark_player, nb_games=20000,\n",
    "                   games_between_updates=500,\n",
    "                   seed=666):\n",
    "    \"\"\"\n",
    "    Plays a given number of games between two players, and returns the rewards.\n",
    "    --dqn_player: Instance of DQNPlayer to train;\n",
    "    --benchmark_player: Player object implementing act();\n",
    "    --nb_games: How many games should be played;\n",
    "    --games_between_updates: how many games are played between two updates of the agent's\n",
    "        target network.\n",
    "    --seed: random seed.\n",
    "    Returns two arrays: rewards, losses\n",
    "    \"\"\"\n",
    "    rewards, losses = [], []\n",
    "    env = TictactoeEnv()\n",
    "\n",
    "    for game in trange(nb_games):\n",
    "        # Sets up the environment for the game\n",
    "        env.reset()\n",
    "        grid, _, _ = env.observe()\n",
    "\n",
    "        # Switch turns\n",
    "        dqn_player.set_player(j=game)\n",
    "        benchmark_player.set_player(j=game + 1)\n",
    "\n",
    "        # Convert the grid from the env's format to that expected by the agent\n",
    "        grid_tensor = grid_to_tensor(grid, env)\n",
    "\n",
    "        game_losses = []\n",
    "\n",
    "        while True:\n",
    "            # Action step\n",
    "            # We now need to account for the case where the agent chooses\n",
    "            # an unavailable position.\n",
    "            try:\n",
    "                if env.current_player == dqn_player.player:\n",
    "                    move = dqn_player.act(grid_tensor)\n",
    "                else:\n",
    "                    move = benchmark_player.act(grid)\n",
    "\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "                grid_tensor = grid_to_tensor(grid, env)\n",
    "                reward = env.reward(dqn_player.player)\n",
    "\n",
    "            except ValueError:\n",
    "                # Being here means the agent chose an impossible action\n",
    "                # Stop the game and set the reward for the agent to -1\n",
    "                end = True\n",
    "                reward = -1\n",
    "\n",
    "            # Learning step\n",
    "            # The DQN agent must have played at least once to start learning\n",
    "            if env.current_player == dqn_player.player or end:\n",
    "                # For the DQN agent, final states will be represented as None\n",
    "                if end:\n",
    "                    grid_tensor = None\n",
    "                game_losses.append(dqn_player.learn(torch.tensor([reward], device=device),\n",
    "                                               grid_tensor))\n",
    "\n",
    "            if end:\n",
    "                losses.append(np.mean(game_losses))\n",
    "                rewards.append(reward)\n",
    "                break\n",
    "\n",
    "        # Update the agent's target network if required\n",
    "        if game % games_between_updates == 0:\n",
    "            dqn_player.update()\n",
    "\n",
    "    return np.array(rewards), np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230333e9-692a-45bc-8112-4a0bc2e413d1",
   "metadata": {},
   "source": [
    "## 3.2 Learning from experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d785c-faf2-48f2-8e28-a67f6a77c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_opt_player = OptimalPlayer(0.5)\n",
    "dqn_player = DQNPlayer(device, epsilon=0.01)\n",
    "\n",
    "# Trains the DQN Agent by playing a semi-optimal player\n",
    "nb_games = 20000\n",
    "rewards, losses = play_games_dqn(dqn_player, semi_opt_player, nb_games=nb_games)\n",
    "\n",
    "# Plots the window-averaged rewards\n",
    "window_size = 250\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1)\n",
    "ax[0].plot(windowed_avg(rewards, window_size))\n",
    "ax[0].set_ylabel('Average final reward over 250 games')\n",
    "ax[0].set_ylim(-1, 1)\n",
    "xticks = np.arange(nb_games // window_size + 1, step=10)\n",
    "labels = [str(k) for k in xticks * window_size]\n",
    "ax[0].set_xticks(xticks)\n",
    "ax[0].set_xticklabels(labels)\n",
    "\n",
    "# Plots average game losses\n",
    "ax[1].plot(windowed_avg(losses, window_size))\n",
    "ax[1].set_ylabel('Average loss over 250 games')\n",
    "ax[1].set_xticks(xticks)\n",
    "ax[1].set_xticklabels(labels)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
